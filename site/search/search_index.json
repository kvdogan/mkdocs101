{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"domain_architecture/","title":"AkerBP Domain Architecture","text":"<pre><code>%%{init: {'theme': 'neutral', 'flowchart': { 'curve': 'linear' } } }%%\ngraph LR\n    Digital[Digital] --&gt; Alvheim\n    Digital --&gt; Ula\n\n    DW[Drilling &amp; Wells]\n    ExpRes[ExpRes]\n    Digital --&gt; Ula\n    FSP[FSP]\n    HSSEQ[HSSEQ]\n    Operations[Operations]\n    Projects[Projects]\n\n    Alvheim[Alvheim]\n    EG[Edvard Grieg]\n    IA[Ivar Aasen]\n    Skarv[Skarv]\n    Ula[Ula]\n    Valhall[Valhall]\n    Yggdrasil[Yggdrasil]\n    OA[Partner Operated Assets]\n\n    analytics[analytics.akerbp.com]\n    docs[docs.akerbp.com] --&gt; Digital\n    ml[ai.akerbp.com]\n    digital[digital.akerbp.com]\n\nsubgraph Assets\n    Valhall\n    Ula\n    EG\n    IA\n    Alvheim\n    Skarv\n    Yggdrasil\n    OA\nend\n\nsubgraph BUs\n    Digital\n    DW\n    ExpRes\n    FSP\n    HSSEQ\n    Operations\n    Projects\nend\n\nsubgraph Domains\n    digital\n    docs\n    analytics\n    ml\nend\n</code></pre>"},{"location":"domain_architecture/#---","title":"---","text":"<pre><code>%%{init: {'theme': 'neutral', 'flowchart' : { 'curve' : 'basis' } } }%%\ngraph LR\n    akerbp[fa:fa-building akerbp.com ] --&gt; Rest\n    akerbp --&gt; Doc\n    digital[fas:fa-book digital.akerbp.com]\n    fabric[fa:fa-question fa:fa-file-code fabric.akerbp.com]\n    express[fa:fa-question fa:fa-file-code express.akerbp.com]\n    digital --&gt; code[fa:fa-question fa:fa-file-code code.akerbp.com]\n    digital --&gt; ml[fa:fa-question fa:fa-file-code ml.akerbp.com]\n    digital --&gt; projects[fa:fa-folder-open projects.akerbp.com]\n    code --&gt; codesubdomains[fa:fa-water /express&lt;br&gt; fa:fa-laptop-code /DIG &lt;br&gt;fa:fa-industry /vallhall]\n    projects --&gt; projectsubdomains[fa:fa-water /express&lt;br&gt; fa:fa-ship /D&amp;W &lt;br&gt;fa:fa-industry /yggdrasil]\n    ml --&gt; mlsubdomains[fa:fa-water /express&lt;br&gt; fa:fa-wrench /maintenance &lt;br&gt;fa:fa-industry /alvheim]\n\nsubgraph Doc[fas:fa-globe Documentation Domains]\n    digital\n    ml\n    code\n    projects\n    codesubdomains\n    projectsubdomains\n    mlsubdomains\nend\nsubgraph Rest[Other BUs]\n    B(...)\n    analytics[fa:fa-file-code analytics.akerbp.com]\n    fabric\n    express\nend\n\n</code></pre>"},{"location":"code-reference/IO_utils/","title":"IO utils","text":""},{"location":"code-reference/IO_utils/#aker_utilities.IO_utils.combine_txt_files","title":"combine_txt_files","text":"<pre><code>combine_txt_files(folder, sep='\\t', encoding='latin1', skip_rows=0)\n</code></pre> <p>Combine txt files in a folder into one file, first column is written as file name.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>PathLike</code> <p>Path to folder of files</p> required <code>sep</code> <code>str</code> <p>Separator for reading</p> <code>'\\t'</code> <code>encoding</code> <code>str</code> <p>Encoding of files.</p> <code>'latin1'</code> <code>skip_rows</code> <code>int</code> <p>Skiprows in header repeats in each file.</p> <code>0</code>"},{"location":"code-reference/IO_utils/#aker_utilities.IO_utils.compare_data_structures","title":"compare_data_structures","text":"<pre><code>compare_data_structures(base, reference, output_folder=None, **kwargs)\n</code></pre> <p>summary</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>tuple[str, dict[str, Any] | list]</code> <p>Tuple of dataset name and data structure either one of dictionary or list</p> required <code>reference</code> <code>tuple[str, dict[str, Any] | list]</code> <p>Tuple of dataset name and data structure either one of dictionary or list</p> required <code>output_folder</code> <code>Path | None</code> <p>Path to export folder. Defaults to None.</p> <code>None</code> <code>kwargs</code> <code>dict[str, Any]</code> <p>Additional arguments to pass to DD i.e. ignore_type_in_groups=[(str, int)]</p> <code>{}</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>description</p> <code>TypeError</code> <p>description</p> <code>AssertionError</code> <p>description</p> <p>Returns:</p> Name Type Description <code>DeepDiff</code> <code>DeepDiff</code> <p>DeepDiff dictionary in any case</p>"},{"location":"code-reference/IO_utils/#aker_utilities.IO_utils.json_to_csv","title":"json_to_csv","text":"<pre><code>json_to_csv(input_file_path, output_file_path, sep=';')\n</code></pre> <p>summary</p> <p>Parameters:</p> Name Type Description Default <code>input_file_path</code> <code>PathLike</code> <p>Full path to input json file. Json objects must be array</p> required <code>output_file_path</code> <code>PathLike</code> <p>Full path to output csv file</p> required <code>sep</code> <code>str</code> <p>Delimiter, Defaults to ';'</p> <code>';'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>description</p> <p>Returns:</p> Name Type Description <code>_type_</code> <code>None</code> <p>description</p>"},{"location":"code-reference/IO_utils/#aker_utilities.IO_utils.read_csv_to_lol","title":"read_csv_to_lol","text":"<pre><code>read_csv_to_lol(full_path, sep=';')\n</code></pre> <p>Read csv file into lists of list. Make sure to have a empty line at the bottom</p> <p>Parameters:</p> Name Type Description Default <code>full_path</code> <code>PathLike</code> <p>Full path to csv file</p> required <code>sep</code> <code>str</code> <p>Seperator string. Defaults to \";\".</p> <code>';'</code> <p>Returns:</p> Type Description <code>list[list[str]]</code> <p>list[list[str]]:</p>"},{"location":"code-reference/IO_utils/#aker_utilities.IO_utils.read_from_txt","title":"read_from_txt","text":"<pre><code>read_from_txt(file_path)\n</code></pre> <p>Read txt file_path as a list, each line becomes list item.</p>"},{"location":"code-reference/IO_utils/#aker_utilities.IO_utils.write_dict_to_yaml","title":"write_dict_to_yaml","text":"<pre><code>write_dict_to_yaml(source_dict, export_path, mapping=2, sequence=4, offset=2)\n</code></pre> <p>summary</p> <p>Parameters:</p> Name Type Description Default <code>source_dict</code> <code>dict</code> <p>Python dict to export to yaml file.</p> required <code>export_path</code> <code>str</code> <p>Export path to create file</p> required <code>mapping</code> <code>int</code> <p>Whitespace number before dict items. Defaults to 2.</p> <code>2</code> <code>sequence</code> <code>int</code> <p>Whitespace number before the key of list item                       (min: offset + 2 is default). Defaults to 4.</p> <code>4</code> <code>offset</code> <code>int</code> <p>Whitespace number before \"dash(-)\" of list item.                     Defaults to 2.</p> <code>2</code>"},{"location":"code-reference/IO_utils/#aker_utilities.IO_utils.write_list_to_txt","title":"write_list_to_txt","text":"<pre><code>write_list_to_txt(source_list, full_path_txt)\n</code></pre> <p>summary</p> <p>Parameters:</p> Name Type Description Default <code>source_list</code> <code>list</code> <p>description</p> required <code>full_path_txt</code> <code>PathLike</code> <p>description</p> required"},{"location":"code-reference/akerbp_datasets/","title":"Akerbp datasets","text":""},{"location":"code-reference/algorithm_utils/","title":"Algorithm utils","text":""},{"location":"code-reference/algorithm_utils/#aker_utilities.algorithm_utils.hierarchy_tree","title":"hierarchy_tree","text":"<pre><code>hierarchy_tree(table, visualize=False, output_file=None)\n</code></pre> <p>Creates a dictionary to represent hierarchical data</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>list[tuple[Any, Any]]</code> <p>description</p> required <code>visualize</code> <code>bool</code> <p>description. Defaults to False.</p> <code>False</code> <code>output_file</code> <code>str | None</code> <p>description. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[Any, Any] | None</code> <p>dict[Any, Any] | None: description</p> <p>Examples:</p> <p>hierarchy_tree( ...     table=[ ...         (22, 4), (45, 1), (1, 1), (4, 4), (566, 45), (7, 7), (66, 1), ...         (300, 8), (8, 4), (101, 7), (80, 22), (17, 17), (911, 66) ...     ] ... ) {1: {66, 45}, 4: {8, 22}, 7: {101}, 8: {300}, 22: {80}, 45: {566}, 66: {911}}</p>"},{"location":"code-reference/algorithm_utils/#aker_utilities.algorithm_utils.natural_sorted","title":"natural_sorted","text":"<pre><code>natural_sorted(alphanum_list)\n</code></pre> <p>When you try to sort a list of strings that contain numbers, the normal python sort algorithm sorts lexicographically, so you might not get the results that you expect.</p> <p>Code is taken from: https://stackoverflow.com/questions/4836710/</p> <p>Parameters:</p> Name Type Description Default <code>alphanum_list</code> <code>list</code> <p>description</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>description</p> <p>Examples:</p> <p>sorted(['2 ft 7 in', '1 ft 5 in', '10 ft 2 in', '2 ft 11 in', '7 ft 6 in']) ['1 ft 5 in', '10 ft 2 in', '2 ft 11 in', '2 ft 7 in', '7 ft 6 in']</p> <p>natural_sorted(['2 ft 7 in', '1 ft 5 in', '10 ft 2 in', '2 ft 11 in', '7 f 6 in']) ['1 ft 5 in', '2 ft 7 in', '2 ft 11 in', '7 f 6 in', '10 ft 2 in']</p>"},{"location":"code-reference/algorithm_utils/#aker_utilities.algorithm_utils.sorted_dict","title":"sorted_dict","text":"<pre><code>sorted_dict(d, reverse=False)\n</code></pre> <p>Sorts a dictionary by its keys</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>description</p> required <code>reverse</code> <code>bool</code> <p>description. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>description</p> <p>Examples:</p> <p>sorted_dict({'a': 1, 'b': 2, 'c': 3, 'd': 4})</p> <p>sorted_dict({'a': 1, 'b': 2, 'c': 3, 'd': 4}, reverse=True)</p>"},{"location":"code-reference/api_extractor/","title":"Api extractor","text":""},{"location":"code-reference/api_extractor/#aker_utilities.api_extractor.Extractor","title":"Extractor","text":"<pre><code>Extractor(api_key, auth_keyword=None, data_keywords=None, page_keywords=None, is_odata=False)\n</code></pre> <p>summary</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>description</p> required <code>auth_keyword</code> <code>str | None</code> <p>Defaults: \"Ocp-Apim-Subscription-Key\" Examples: \"Ocp-Apim-Subscription-Key\" for APIM, \"Authorization\" for SAP PRDFIORI</p> <code>None</code> <code>data_keywords</code> <code>list[str] | None</code> <p>Defaults: [\"d\", \"results\"]</p> <code>None</code> <code>page_keywords</code> <code>tuple[str, str] | None</code> <p>Defaults: (\"\\(skip\", \"\\)top\")</p> <code>None</code>"},{"location":"code-reference/api_extractor/#aker_utilities.api_extractor.Extractor.get_columns","title":"get_columns","text":"<pre><code>get_columns(endpoint_tuple)\n</code></pre> <p>Return table columns for given list of api endpoints in alphabetical order. List</p> <p>Parameters:</p> Name Type Description Default <code>list_of_endpoint_tuples</code> <code>list[tuple[str, str, str, str, str, str]]</code> required Example <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: description</p>"},{"location":"code-reference/api_extractor/#aker_utilities.api_extractor.Extractor.get_columns--for-single-table-extraction","title":"For single table extraction","text":"<p>apim.extract_columns_of_single_endpoint(    [APIM_SAP_ENDPOINTS[\"functional_location_object_type\"]] )</p>"},{"location":"code-reference/api_extractor/#aker_utilities.api_extractor.Extractor.get_columns_from_multiple_endpoints","title":"get_columns_from_multiple_endpoints","text":"<pre><code>get_columns_from_multiple_endpoints(list_of_endpoint_tuples)\n</code></pre> <p>Return table columns for given list of api endpoints in alphabetical order. List</p> <p>Parameters:</p> Name Type Description Default <code>list_of_endpoint_tuples</code> <code>list[tuple[str, str, str, str, str, str]]</code> required Example <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>dict[str, list[str]]: description</p>"},{"location":"code-reference/api_extractor/#aker_utilities.api_extractor.Extractor.get_columns_from_multiple_endpoints--for-multiple-table-extraction","title":"For multiple table extraction","text":"<p>apim.get_columns_from_multiple_endpoints(     list(APIM_SAP_ENDPOINT.values()) )</p>"},{"location":"code-reference/api_extractor/#aker_utilities.api_extractor.Extractor.get_records","title":"get_records","text":"<pre><code>get_records(endpoint_tuple, odata_query=None, limit=None)\n</code></pre> <p>Return desired amount of records for given api endpoints.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint_tuple</code> <code>tuple[str, str, str]</code> <p>description</p> required <code>limit</code> <code>int</code> <p>Default to 25 in SDK, set &lt;= 0 for no limit</p> <code>None</code> <p><pre><code>APIM_SAP_ENDPOINTS: dict[str, tuple[str, str, str]] = dict(\n    barrier_hierarchy_maintenance_get_barrier_hierarchies=(\n        \"https://gateway.api.akerbp.com/barrier-hierarchy-maintenance/v1/ZEAM...\",\n        \"barrier-hierarchy-maintenance-get-barrier-hierarchies\",\n        \"{baCharLvl1}-{baCharLvl2}-{baCharLvl3}-{baCharLvl4}-{baCharLvl5}\",\n    ),\n    characteristics_maintenance_characteristics=(\n        \"https://gateway.api.akerbp.com/characteristics-maintenance/v1/ZEAM_C...\",\n        \"characteristics-maintenance-characteristics\",\n        \"{characteristic}-{characteristicValue}\",\n    ),\n</code></pre> Example:     # For single table extraction     &gt;&gt;&gt; apim.get_records([APIM_SAP_ENDPOINTS[\"functional-location-object-type\"]])</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dict[str, pd.DataFrame]: description</p>"},{"location":"code-reference/api_extractor/#aker_utilities.api_extractor.Extractor.get_records_from_multiple_endpoints","title":"get_records_from_multiple_endpoints","text":"<pre><code>get_records_from_multiple_endpoints(list_of_endpoint_tuples, list_of_odata_query=None, limit=None)\n</code></pre> <p>Return table records for given list of api endpoints in alphetical order. List</p> <p>Parameters:</p> Name Type Description Default <code>list_of_endpoint_tuples</code> <code>list[tuple[str, str, str]]</code> required <code>limit</code> <code>int | None</code> <p>Default to 25 in SDK, set &lt;= 0 for no limit</p> <code>None</code> Example <p>Returns:</p> Type Description <code>dict[str, DataFrame]</code> <p>dict[str, pd.DataFrame]: description</p>"},{"location":"code-reference/api_extractor/#aker_utilities.api_extractor.Extractor.get_records_from_multiple_endpoints--for-multiple-table-extraction","title":"For multiple table extraction","text":"<p>apim.get_records_from_multiple_endpoints(    list(APIM_SAP_ENDPOINT.values()), limit=1 )</p>"},{"location":"code-reference/automation_utils/","title":"Automation utils","text":""},{"location":"code-reference/automation_utils/#aker_utilities.automation_utils.keep_mouse_busy","title":"keep_mouse_busy","text":"<pre><code>keep_mouse_busy()\n</code></pre> <p>Draw rectangle with mouse pointer to keep pc awake and responsive.</p>"},{"location":"code-reference/az_devops/","title":"Az devops","text":""},{"location":"code-reference/az_devops/#aker_utilities.az_devops.DevOpsSDK","title":"DevOpsSDK","text":"<pre><code>DevOpsSDK(organization, project, url=None, pat=None)\n</code></pre>"},{"location":"code-reference/az_devops/#aker_utilities.az_devops.DevOpsSDK--from-azuredevopsv7_0git-import-git_client_base--this-is-for-codebase-ref-only","title":"from azure.devops.v7_0.git import git_client_base  # This is for codebase ref only","text":""},{"location":"code-reference/az_devops/#aker_utilities.az_devops.DevOpsSDK--from-azuredevopsv7_0core-import-core_client--this-is-for-codebase-ref-only","title":"from azure.devops.v7_0.core import core_client     # This is for codebase ref only","text":""},{"location":"code-reference/build_hierarchy_tree/","title":"Build hierarchy tree","text":""},{"location":"code-reference/build_hierarchy_tree/#aker_utilities.build_hierarchy_tree.main","title":"main","text":"<pre><code>main()\n</code></pre> <p>summary: This function is a GUI wrapper over outlined_hierarchy and hierarchy_tree functions which are used to build hierarchy tree and export to outline excel file from a three column csv file. Columns order should be like: tag; tag_description; parent tag</p>"},{"location":"code-reference/bundle_single_html/","title":"Bundle single html","text":""},{"location":"code-reference/bundle_single_html/#aker_utilities.bundle_single_html.generate","title":"generate","text":"<pre><code>generate(index, verbose=True, comment=True, keep_script=False, prettify=False, full_url=True, verify=True, errorpage=False, username=None, password=None, **kwargs)\n</code></pre> <p>given a index url such as http://www.google.com, http://custom.domain/index.html return generated single html</p>"},{"location":"code-reference/bundle_single_html/#aker_utilities.bundle_single_html.generate--usage-in-code","title":"Usage in code","text":"<p>html = generate(     r\"C:/Users/kahdog/OneDrive - Aker BP/Desktop/SAPFull/index.html\",     comment=False,     keep_script=True,     prettify=True )</p> <p>with open(     \"C:/Users/kahdog/OneDrive - Aker BP/Desktop/SAPFull/indexx.html\", \"w\",     encoding=\"utf-8\" ) as ff:     ff.write(html)</p>"},{"location":"code-reference/cdf_utils/","title":"Cdf utils","text":""},{"location":"code-reference/cdf_utils/#aker_utilities.cdf_utils.CDF","title":"CDF","text":"<pre><code>CDF(client_name, project, client_id, tenant_id, client_secret=None, port=53000)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>client_name</code> <code>str</code> <p>description</p> required <code>project</code> <code>Literal['abp', 'abp-dev', 'abp-test', 'akerbp', 'akerbp-dev', 'akerbp-test', 'akerbp-sandbox']</code> <p>Should be one of the given AkerBP tenant</p> required <code>client_id</code> <code>str</code> <p>Client ID for Azure AD</p> required <code>tenant_id</code> <code>str</code> <p>Tenant ID for Azure AD</p> required <code>client_secret</code> <code>str | None</code> <p>For non interactive auth</p> <code>None</code> <code>port</code> <code>int</code> <p>Defaults to 53000.</p> <code>53000</code>"},{"location":"code-reference/cdf_utils/#aker_utilities.cdf_utils.CDF.create_model_transformation","title":"create_model_transformation","text":"<pre><code>create_model_transformation(list_of_transformation_dict)\n</code></pre> <p>Create transformation for given list of transformation dictionaries</p> Example <pre><code>list_of_transformation_dict = [\n    dict(\n            space=\"E2E\",\n            model=\"SAP\",\n            type_name=\"Asset\",\n            version=\"1\",\n            trans_external_id=\"E2E:Asset_1\",\n            trans_name=\"E2E:Asset_1\",\n            query='''\n    SELECT DISTINCT\n    concat_ws(\":\", \"E2E\", \"Asset\", `flocMainAsset`) as `externalId`,\n    string(`flocMainAsset`) as `flocMainAsset`\n    FROM `e2e-maintenance-sap`.`functional-location-functional-location-metadata`;\n    ''',\n            notification=\"kahraman.v.dogan@akerbp.com\",\n            data_set_id=1613619045197736    # dataset:e2e-maintenance-sap\n        ),\n]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_transformation_list</code> <code>list[dict[Any, Any]]</code> <p>description</p> required"},{"location":"code-reference/cdf_utils/#aker_utilities.cdf_utils.CDF.create_transformation_notifications","title":"create_transformation_notifications","text":"<pre><code>create_transformation_notifications(list_of_transformation_dict)\n</code></pre> <p>Creates transformation notifications for given list of transformation dictionaries</p> <p>Parameters:</p> Name Type Description Default <code>list_of_transformation_dict</code> <code>list[dict[str, str]]</code> <p>description</p> required"},{"location":"code-reference/cdf_utils/#aker_utilities.cdf_utils.CDF.delete_instances_views_containers_for_given_model","title":"delete_instances_views_containers_for_given_model","text":"<pre><code>delete_instances_views_containers_for_given_model(space, model_name, list_of_views, chunk_size=100)\n</code></pre> <p>Clean up existing data model instances, views and containers for given model without leaving residues behind</p> <p>Parameters:</p> Name Type Description Default <code>space</code> <code>str</code> <p>Name of the model space</p> required <code>model_name</code> <code>str</code> <p>Name of the model</p> required <code>list_of_views</code> <code>list[str] | None</code> <p>List of views to be deleted</p> required <code>chunk_size</code> <code>int</code> <p>Defaults to 100.</p> <code>100</code>"},{"location":"code-reference/cdf_utils/#aker_utilities.cdf_utils.CDF.delete_records","title":"delete_records","text":"<pre><code>delete_records(db_name, table_name, key)\n</code></pre> <p>summary</p> <p>Parameters:</p> Name Type Description Default <code>db_name</code> <code>str</code> <p>description</p> required <code>table_name</code> <code>str</code> <p>description</p> required <code>key</code> <code>list[str]</code> <p>description</p> required"},{"location":"code-reference/cdf_utils/#aker_utilities.cdf_utils.CDF.delete_tables","title":"delete_tables","text":"<pre><code>delete_tables(db_name, tables)\n</code></pre> <p>summary</p> <p>Parameters:</p> Name Type Description Default <code>db_name</code> <code>str</code> <p>description</p> required <code>tables</code> <code>str | list[str] | None</code> <p>description</p> required <p>Raises:</p> Type Description <code>NameError</code> <p>description</p>"},{"location":"code-reference/cdf_utils/#aker_utilities.cdf_utils.CDF.delete_transformation_notifications","title":"delete_transformation_notifications","text":"<pre><code>delete_transformation_notifications(list_of_transformation_dict)\n</code></pre> <p>Deletes transformation notifications for given list of transformation dictionaries</p> <p>Parameters:</p> Name Type Description Default <code>list_of_transformation_dict</code> <code>list[dict[str, str]]</code> <p>description</p> required"},{"location":"code-reference/cdf_utils/#aker_utilities.cdf_utils.CDF.get_columns","title":"get_columns","text":"<pre><code>get_columns(db_name, table_name)\n</code></pre> <p>summary</p> <p>Parameters:</p> Name Type Description Default <code>db_name</code> <code>str</code> <p>description</p> required <code>table_name</code> <code>str</code> <p>description</p> required"},{"location":"code-reference/cdf_utils/#aker_utilities.cdf_utils.CDF.get_columns_from_multiple_tables","title":"get_columns_from_multiple_tables","text":"<pre><code>get_columns_from_multiple_tables(db_name, tables)\n</code></pre> <p>summary</p> <p>Parameters:</p> Name Type Description Default <code>db_name</code> <code>str</code> <p>description</p> required <code>tables</code> <code>str | list | None</code> <p>description</p> required <p>Raises:</p> Type Description <code>NameError</code> <p>description</p> <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>dict[str, list[str]]: description</p>"},{"location":"code-reference/cdf_utils/#aker_utilities.cdf_utils.CDF.get_records","title":"get_records","text":"<pre><code>get_records(db_name, table_name, min_last_updated_time=None, max_last_updated_time=None, columns=None, limit=None)\n</code></pre> <p>summary</p> <p>Parameters:</p> Name Type Description Default <code>db_name</code> <code>str</code> <p>description</p> required <code>table_name</code> <code>str</code> <p>description</p> required <code>min_last_updated_time</code> <code>int | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>max_last_updated_time</code> <code>int | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>columns</code> <code>list[str] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>limit</code> <code>int | None</code> <p>description. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: description</p>"},{"location":"code-reference/cdf_utils/#aker_utilities.cdf_utils.CDF.get_records_from_multiple_tables","title":"get_records_from_multiple_tables","text":"<pre><code>get_records_from_multiple_tables(db_name, tables=None, min_last_updated_time=None, max_last_updated_time=None, columns=None, limit=None)\n</code></pre> <p>summary</p> <p>Parameters:</p> Name Type Description Default <code>db_name</code> <code>str</code> <p>description</p> required <code>tables</code> <code>list[str] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>min_last_updated_time</code> <code>int | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>max_last_updated_time</code> <code>int | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>columns</code> <code>list[str] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>limit</code> <code>int | None</code> <p>description. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>description</p> <p>Returns:</p> Type Description <code>dict[str, DataFrame]</code> <p>dict[str, pd.DataFrame]: description</p>"},{"location":"code-reference/cdf_utils/#aker_utilities.cdf_utils.CDF.get_stale_records","title":"get_stale_records","text":"<pre><code>get_stale_records(db_name, tables, field, records_before, export_details=False)\n</code></pre> <p>This function is used to find stale records in a given list of tables in a given database. By stale records we mean records that are older than the latest record in the given table identified by _ingestionDT datetime field.</p> <p>Datetime filter for stale records is the older one among the max _ingestionDT and given records_before datetime argument. 6 hours of buffer is added to selection.</p> <p>Export details option is used to export all the keys with ingestionDT for each</p> <p>Parameters:</p> Name Type Description Default <code>db_name</code> <code>str</code> <p>description</p> required <code>tables</code> <code>list[str] | None</code> <p>description</p> required <code>field</code> <code>str</code> <p>description</p> required <code>records_before</code> <code>datetime</code> <p>description</p> required <code>export_details</code> <code>bool</code> <p>description. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, list[str | None]]</code> <p>dict[str, list[str | None]]: description</p>"},{"location":"code-reference/cdf_utils/#aker_utilities.cdf_utils.CDF.get_tables","title":"get_tables","text":"<pre><code>get_tables(db_name)\n</code></pre> <p>List all the tables in alphetical order for the given database in CDF.</p> <p>Parameters:</p> Name Type Description Default <code>dbname</code> <code>str</code> <p>Name of the database i.e. e2e-maintenance-sap</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>description</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: description</p>"},{"location":"code-reference/cdf_utils/#aker_utilities.cdf_utils.CDF.get_transformation_preview","title":"get_transformation_preview","text":"<pre><code>get_transformation_preview(query)\n</code></pre> <p>Pay attention to array types because those can not be converted json, however in transformation itself you should not use to_json method unless you want json in the column value.</p> Example <pre><code>query = '''\nSelect\n    concat_ws(':', 'sap', 'planningPlant', `plantPlanningPlant`) as `externalId`,\n    string(`plantPlanningPlant`) as `plantPlanningPlant`,\n    collect_set(`plantPlanningPlantDesc`)[0] as `plantPlanningPlantDesc`,\n    to_json(collect_set(`plantSystemCondition`)) as `plantSystemCondition`,\n    to_json(collect_set(`plantOperationalConsequence`)) as `plantOperationalCons`\nFROM `e2e-maintenance-sap`.`planning-plants-maintenance-ssd-codes`\nGROUP BY `plantPlanningPlant`;'''\n</code></pre>"},{"location":"code-reference/cdf_utils/#aker_utilities.cdf_utils.CDF.get_transformation_schemas","title":"get_transformation_schemas","text":"<pre><code>get_transformation_schemas(list_of_transformation_dict)\n</code></pre> <p>Get transformation schemas for given list of transformation dictionaries</p> <p>Args:      data_model_instances (list[tuple[str, str, str]]):      [(model_id, space_id, instance_space_id), ... ]</p>"},{"location":"code-reference/cdf_utils/#aker_utilities.cdf_utils.CDF.get_transformations","title":"get_transformations","text":"<pre><code>get_transformations(destination_type=None, owner=None)\n</code></pre> <p>Get list of transformations based on destination_type and user email arguments</p> Example <pre><code>tslist = get_transformations(\n    destination_type=\"data_model_instances\",\n    owner=\"kahraman.v.dogan@akerbp.com\"\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>destination_type</code> <code>str | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>owner</code> <code>str | None</code> <p>description. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>TransformationList | list[Transformation]</code> <p>description</p>"},{"location":"code-reference/cdf_utils/#aker_utilities.cdf_utils.CDF.run_transformation","title":"run_transformation","text":"<pre><code>run_transformation(list_of_transformation_dict)\n</code></pre> <p>Run transformation for given list of transformation dictionaries</p> <p>Parameters:</p> Name Type Description Default <code>list_of_transformation_dict</code> <code>list[dict[str, str]]</code> <p>description</p> required"},{"location":"code-reference/cdf_utils/#aker_utilities.cdf_utils.CDF.update_or_create_model_transformation","title":"update_or_create_model_transformation","text":"<pre><code>update_or_create_model_transformation(list_of_transformation_dict, create=False)\n</code></pre> <p>Name must be set to new name to update, external_id must be existing name, after update external_id will be set as name, so both will be synced with given as name</p> <p>Important</p> <p>Reason for using update_or_create_node_transformation rather than create_model_transformation upsert functionality is to be able to update external_id</p> <p>So that external_id and model_instance version are at par.</p> <p>Parameters:</p> Name Type Description Default <code>list_of_transformation_dict</code> <code>list[dict[str, str]]</code> <p>description</p> required"},{"location":"code-reference/cdf_utils/#aker_utilities.cdf_utils.PipelineToCDF","title":"PipelineToCDF","text":"<pre><code>PipelineToCDF(source, endpoint, uid_key_list, cdf, cdf_db_name, cdf_table, limit=None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Pipeline to CDF class to push data to CDF from different sources.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Extractor | Path</code> <p>Path to the file or Extractor object</p> required <code>endpoint</code> <code>tuple[str, str, str] | None</code> <p>Required if source is Extractor</p> required <code>uid_key_list</code> <code>str | list[str] | None</code> <p>if None, a unique key will be generated</p> required <code>cdf</code> <code>CDF</code> <p>CDF object</p> required <code>cdf_db_name</code> <code>str</code> <p>CDF database name</p> required <code>cdf_table</code> <code>str</code> <p>CDF table name</p> required <code>limit</code> <code>int</code> <p>Number of rows to read via Extractor. Defaults to 0.</p> <code>None</code>"},{"location":"code-reference/confluence/","title":"Confluence","text":""},{"location":"code-reference/excel_utils/","title":"Excel utils","text":""},{"location":"code-reference/excel_utils/#aker_utilities.excel_utils.combine_multiple_csv_into_excel","title":"combine_multiple_csv_into_excel","text":"<pre><code>combine_multiple_csv_into_excel(full_path_to_folder, sep='\\t', encoding='latin1')\n</code></pre> <p>Combine csv files that can be converted to Dataframe and have same exact structure. :param full_path_to_folder: :param sep: Text separator, default is '\\t' :param encoding: Text encoding, default is 'latin1' :return: excel file with one extra column showing the name of the file.</p>"},{"location":"code-reference/excel_utils/#aker_utilities.excel_utils.convert_to_hyperlink","title":"convert_to_hyperlink","text":"<pre><code>convert_to_hyperlink(x)\n</code></pre> <p>Converts pandas column given in apply function to hyperlink for excel export</p> Usage <p>df['Link'] = df['Link'].apply(convert_to_hyperlink)</p> <p>Returns:</p> Type Description <p>DataFrame -- Creates or modifies given DataFrame column</p>"},{"location":"code-reference/excel_utils/#aker_utilities.excel_utils.export_file_names","title":"export_file_names","text":"<pre><code>export_file_names(file_type=None, use_relative_path=True)\n</code></pre> <p>Walk through the folder structures and creates excel file with hyperlink. to every single file</p> <p>Other Parameters:</p> Name Type Description <code>file_type</code> <code>{string} -- File extension i.e. \".xls\" (default</code> <p>{None})</p> <code>use_relative_path</code> <code>{bool} -- Relative paths to selected folder (default</code> <p>{True})</p>"},{"location":"code-reference/excel_utils/#aker_utilities.excel_utils.outlined_hierarchy","title":"outlined_hierarchy","text":"<pre><code>outlined_hierarchy(txtfile, sysname='HVAC_sample', sysno='97_sample', wkbk='Outlined_Hierarchy_sample.xlsx', ws='Hierarchy')\n</code></pre> <p>Create a hierarchical structure from the given file by looking parent and child Arguments:     txtfile {[type]} -- Structured txt file which is the output of hierarchy tree algo</p> <p>Other Parameters:</p> Name Type Description <code>sysname</code> <code>{str} -- Name of the system (default</code> <p>{\"HVAC_sample\"})</p> <code>sysno</code> <code>{str} -- Number of the system (default</code> <p>{\"97_sample\"})</p> <code>wkbk</code> <code>{str} -- Name for output file (default</code> <p>{\"Outlined_Hierarchy_sample.xlsx\"})</p> <code>ws</code> <code>{str} -- Name for excel sheey (default</code> <p>{\"Hierarchy\"})</p>"},{"location":"code-reference/excel_utils/#aker_utilities.excel_utils.read_excel_to_lol","title":"read_excel_to_lol","text":"<pre><code>read_excel_to_lol(fname, sheet_index=0)\n</code></pre> <p>Read excel file into lists of list. Make sure to indicate sheet index if it is not the first sheet</p>"},{"location":"code-reference/excel_utils/#aker_utilities.excel_utils.split_worksheets","title":"split_worksheets","text":"<pre><code>split_worksheets(file)\n</code></pre> <p>:param file: Excel file to be split by its worksheets. :return:</p>"},{"location":"code-reference/graph_data_model/","title":"Graph data model","text":""},{"location":"code-reference/graph_data_model/#aker_utilities.graph_data_model.GraphqlDataModel","title":"GraphqlDataModel","text":"<pre><code>GraphqlDataModel(path, gen_type_mapping=GEN_TYPE_MAP, nullability_mapping=NULLABLILITY_MAP)\n</code></pre> <p>Using graphql-core library(https://github.com/graphql-python/graphql-core) parse funct Reason for feeding gen_type_mapping is keeping class loosely bound to different type of sql implementation</p>"},{"location":"code-reference/graph_data_model/#aker_utilities.graph_data_model.GraphqlDataModel.datatype_mapping","title":"datatype_mapping  <code>property</code>","text":"<pre><code>datatype_mapping\n</code></pre> <p>Finding custom types implemented in the project and updating gen_type_mapping.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>description</p>"},{"location":"code-reference/graph_data_model/#aker_utilities.graph_data_model.GraphqlDataModel.export_graphql_ast","title":"export_graphql_ast","text":"<pre><code>export_graphql_ast(path)\n</code></pre> <p>Export graphql query as AST(abstract syntax trees) json.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Pathlike object in json extension</p> required"},{"location":"code-reference/graph_data_model/#aker_utilities.graph_data_model.GraphqlDataModel.export_graphql_schema","title":"export_graphql_schema","text":"<pre><code>export_graphql_schema(path, sort=False)\n</code></pre> <p>Export graphql schema in SDL (Schema definition language) to gql file</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Pathlike in gql extension</p> required"},{"location":"code-reference/graph_data_model/#aker_utilities.graph_data_model.GraphqlDataModel.export_type_fields","title":"export_type_fields","text":"<pre><code>export_type_fields(path)\n</code></pre> This method converts graphql model into python dict like <p>{ type_name: [\"attrA\", \"attrB\", ...] }</p> <p>Gets only types, ignores interfaces, however interface attributes exists in types.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike | None</code> <p>If given and it is correct then exports                                  dict as json. Defaults to None.</p> required Example <p>from pathlib import Path gql = gql = GraphqlToSql(Path(\"__ref/gql_utils/advancedDB.gql\")) file_path = Path(\"__ref\", \"gql_utils\", \"advancedDB.json\") gql.export_type_fields(path=file_path)</p> <p>Returns:</p> Type Description <code>None</code> <p>dict | None: description</p>"},{"location":"code-reference/graph_data_model/#aker_utilities.graph_data_model.GraphqlDataModel.get_type_attributes_only","title":"get_type_attributes_only","text":"<pre><code>get_type_attributes_only()\n</code></pre> <p>Exports all unique attributes exists in the model</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike</code> <p>description</p> required Example <p>from pathlib import Path gql = gql = GraphqlToSql(Path(\"__ref/gql_utils/iso.gql\")) file_path = Path(\"__ref\", \"gql_utils\", \"all_ISO_attributes.txt\") gql.export_all_attributes(path=file_path)</p>"},{"location":"code-reference/graph_data_model/#aker_utilities.graph_data_model.GraphqlDataModel.sort_schema","title":"sort_schema","text":"<pre><code>sort_schema()\n</code></pre> <p>Sort Schema lexicographically based on type names</p> <p>Returns:</p> Name Type Description <code>GraphQLSchema</code> <code>GraphQLSchema</code> <p>description</p>"},{"location":"code-reference/pandas_utils/","title":"Pandas utils","text":""},{"location":"code-reference/pandas_utils/#aker_utilities.pandas_utils.dataframe_countif","title":"dataframe_countif","text":"<pre><code>dataframe_countif(df, col)\n</code></pre> <p>:param df1: Dataframe :param col: Column to count</p>"},{"location":"code-reference/pandas_utils/#aker_utilities.pandas_utils.dataframe_diff","title":"dataframe_diff","text":"<pre><code>dataframe_diff(df1, df2)\n</code></pre> <p>Give difference between two pandas dataframe.          Date   Fruit   Num   Color 9  2013-11-25  Orange   8.6  Orange 8  2013-11-25   Apple  22.1     Red</p>"},{"location":"code-reference/pandas_utils/#aker_utilities.pandas_utils.get_hierarchy_as_list","title":"get_hierarchy_as_list","text":"<pre><code>get_hierarchy_as_list(main_df, tag_list, lookup_for='children', exclude_list=None, sub_level=None, tag_column='Functional Location', parent_column='Superior functional location', print_details=True)\n</code></pre> <p>Retrieve parent tags or children tags as a dataframe for a given tag lists</p> <p>Other Parameters:</p> Name Type Description <code>(default</code> <p>{None})</p> <code>sub_level</code> <code>{int} -- Level of parent or children tags (default</code> <p>{None})</p> <code>tag_column</code> <code>{str} -- Name of the tag in main_df (default</code> <p>{'Functional Location'})</p> <code>(default</code> <p>{'Superior functional location'})</p> <p>Returns:</p> Type Description <p>[pandas.Dataframe] -- [Output of desired list of parents or children]</p>"},{"location":"code-reference/pandas_utils/#aker_utilities.pandas_utils.sort_groupby_hierarchical","title":"sort_groupby_hierarchical","text":"<pre><code>sort_groupby_hierarchical(orig_df, export=False)\n</code></pre> <p>Examines all the columns in dataframe and sort by number of unique values and group by in that order, so that it reprensents hierarchical model</p> <p>Parameters:</p> Name Type Description Default <code>orig_df</code> <code>DataFrame</code> <p>description</p> required"},{"location":"code-reference/pandas_utils/#aker_utilities.pandas_utils.split_and_export_dataframe","title":"split_and_export_dataframe","text":"<pre><code>split_and_export_dataframe(df, nrows, sortby=None, output_name=None, export=True)\n</code></pre> <p>Split dataframe with given row numbers and return dict or export to csv</p> <p>Other Parameters:</p> Name Type Description <code>sortby</code> <code>{string} --  (default</code> <p>{None})</p> <code>output_name</code> <code>{string} --  (default</code> <p>{None})</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>_type_</code> <p>description</p> required <code>nrows</code> <code>_type_</code> <p>Number of rows to split the database</p> required <code>sortby</code> <code>_type_</code> <p>Sort DataFrame before splitting. Defaults to None.</p> <code>None</code> <code>output_name</code> <code>_type_</code> <p>Output file name, Defaults to None equal to input</p> <code>None</code> <code>export</code> <code>bool</code> <p>Output file format, ';' seperated csv file'.</p> <code>True</code>"},{"location":"code-reference/path_utils/","title":"Path utils","text":""},{"location":"code-reference/path_utils/#aker_utilities.path_utils.checkfile","title":"checkfile","text":"<pre><code>checkfile(path)\n</code></pre> <p>Check file in the path, and extend with '(1)' like numbering system if there exists That helps to avoid overwriting and existing file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>description</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>description</p>"},{"location":"code-reference/path_utils/#aker_utilities.path_utils.get_filepaths","title":"get_filepaths","text":"<pre><code>get_filepaths(rootdir, file_type=None, flat=True, recursive=True)\n</code></pre> <p>Advanced tool for getting file paths from nested folders.</p> <p>In case of flat argument is True:   ['file1_fullpath', 'file2_fullpath', ....]   ['C:\\temp\\xxx.txt',r'C:\\temp\\temp2\\xxx.txt', ....]</p> <p>In case of flat argument is False:     Returns a list of tuples of filename, full path to folder and index in rootdir</p> <p>[(file1_name, file1_parentfolder_path, file1_peers_number)]   [         (\"xxx.txt\", r'C:\\temp', 2),         ('xxx.txt',r'C:\\temp\\temp2',5)     ]</p> <p>Parameters:</p> Name Type Description Default <code>rootdir</code> <code>str | Path</code> <p>Fullpath to directory, string, default is None.</p> required <code>file_type</code> <code>str | list[str]</code> <p>File extension to look up, string or list,                                    Defaults to None</p> <code>None</code> <code>flat</code> <code>bool</code> <p>Return either flat list consist of fullpath of files from                    nested folders if 'flat' is True. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>description</p> <p>Returns:</p> Type Description <code>list[Path] | list[tuple[str, Path, int]]</code> <p>list[Path] | list[tuple[str, Path, int]]: description</p>"},{"location":"code-reference/path_utils/#aker_utilities.path_utils.get_folder_structure","title":"get_folder_structure","text":"<pre><code>get_folder_structure(path, file_type=None)\n</code></pre> <p>Generates folder structure as a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>description</p> required <code>file_type</code> <code>str | list[str] | None</code> <p>description. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>description</p>"},{"location":"code-reference/path_utils/#aker_utilities.path_utils.is_file_created_within_given_margin","title":"is_file_created_within_given_margin","text":"<pre><code>is_file_created_within_given_margin(path, margin_in_sec, reference_datetime=None, debug=False)\n</code></pre> <p>Check if the last edit date for a file is within given margin by the given reference. If no reference date is given, exact time being will be considered as reference time. Returns True or False</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>[description]</p> required <code>margin_in_sec</code> <code>int</code> <p>Allowed time difference in seconds</p> required <code>reference_datetime</code> <code>datetime</code> <p>[description]. Defaults to None.</p> <code>None</code> <code>debug</code> <code>bool</code> <p>Defaults to False. Prints timediff if needed.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>[description]</p>"},{"location":"code-reference/pdf_utils/","title":"Pdf utils","text":""},{"location":"code-reference/pdf_utils/#aker_utilities.pdf_utils.PDFEditor","title":"PDFEditor","text":"<pre><code>PDFEditor(path)\n</code></pre> <p>               Bases: <code>object</code></p>"},{"location":"code-reference/pdf_utils/#aker_utilities.pdf_utils.PDFEditor.convert_page","title":"convert_page","text":"<pre><code>convert_page(page_no, option='html', sort=True, include_images=False, save_as=None)\n</code></pre> <p>Extract given pdf page as given format with or without images.</p> <p>Parameters:</p> Name Type Description Default <code>page_no</code> <code>int</code> <p>Page number as in pdf. (1-index)</p> required <code>option</code> <code>str</code> <p>Conversion format, [\"html\", \"word\"] Defaults to \"html\"</p> <code>'html'</code> <code>sort</code> <code>bool</code> <p>Fix the order of the document for auto generated PDFs</p> <code>True</code> <code>include_images</code> <code>bool</code> <p>description. Defaults to False.</p> <code>False</code> <code>export_path</code> <code>str</code> <p>In case of export. Defaults to None.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str | None</code> <p>description</p>"},{"location":"code-reference/pdf_utils/#aker_utilities.pdf_utils.PDFEditor.crop","title":"crop","text":"<pre><code>crop(cropbox, pages=None, save_as=None)\n</code></pre> <p>Crops pages in given pdf with given crop box and saves as new pdf</p> <p>Parameters:</p> Name Type Description Default <code>pages</code> <code>list[int] | None</code> <p>Page numbers as in pdf. (1-index), def to all pages</p> <code>None</code> <code>cropbox</code> <code>tuple[int, int, int, int]</code> <p>Corner coordinates of top_left and                                   bottom right of an crop-box rectangle.</p> required <code>save_as</code> <code>str | None</code> <p>description</p> <code>None</code>"},{"location":"code-reference/pdf_utils/#aker_utilities.pdf_utils.PDFEditor.delete","title":"delete","text":"<pre><code>delete(page_range, save_as)\n</code></pre> <p>Delete pages out of given pdf page range and save_as with different name. If deleted page have bookmark, keep it, otherwise no bookmark.</p> <p>If page_range is: 1. tuple of two int : Split page range either single pdfs or one pdf 2. list of integers : Split given arbitrary page numbers either single pdfs                       or one pdf 3. None exports all thee page</p> <p>Parameters:</p> Name Type Description Default <code>page_range</code> <code>tuple[int, int] | list[int] | None</code> <p>1-Index</p> required <code>single_pages</code> <code>bool</code> <p>description. Defaults to False.</p> required <p>Example CLI:</p>"},{"location":"code-reference/pdf_utils/#aker_utilities.pdf_utils.PDFEditor.delete--exports-all-pages-as-single-page-pdf","title":"Exports all pages as single page pdf","text":"<p>pdf_tool --path=\"./sample.pdf\" delete</p>"},{"location":"code-reference/pdf_utils/#aker_utilities.pdf_utils.PDFEditor.export","title":"export","text":"<pre><code>export(page_range=None, single_pages=True, save_as=None)\n</code></pre> <p>Export pages out of given pdf page range either as one pdf or many single page If exported page have bookmark, keep it, otherwise no bookmark.</p> <p>If page_range is: 1. tuple of two int : Split page range either single pdfs or one pdf 2. list of integers : Split given arbitrary page numbers either single pdfs                       or one pdf 3. None exports all thee page</p> <p>Parameters:</p> Name Type Description Default <code>page_range</code> <code>tuple[int, int] | list[int] | None</code> <p>1-Index</p> <code>None</code> <code>single_pages</code> <code>bool</code> <p>description. Defaults to False.</p> <code>True</code> <p>Example CLI:</p>"},{"location":"code-reference/pdf_utils/#aker_utilities.pdf_utils.PDFEditor.export--exports-all-pages-as-single-page-pdf","title":"Exports all pages as single page pdf","text":"<p>pdf_tool export --path=\"./sample.pdf\"</p>"},{"location":"code-reference/pdf_utils/#aker_utilities.pdf_utils.PDFEditor.insert","title":"insert","text":"<pre><code>insert(path_to_insert, page_range=None, start_at=None, rotate=0, save=False)\n</code></pre> <p>Insert pdf, pages in any position, can also be used as append if start_at is not given (Append by default), smartly handles table of content</p> <p>Parameters:</p> Name Type Description Default <code>path_to_insert</code> <code>str</code> <p>PDF to copy from. Must be different object,            but may be same file.</p> required <code>page_range</code> <code>Tuple[int, int]</code> <p>First source page to copy,                                     1-index, default 1.                                     Last source page to copy,                                     1-index, default last.</p> <code>None</code> <code>start_at</code> <code>int</code> <p>1-index and it'll become this page number in target</p> <code>None</code> <code>rotate</code> <code>int</code> <p>rotate copied pages, default 0 is no change.</p> <code>0</code> <code>save</code> <code>str</code> <p>Defaults to False.                   True for overwriting original file                   False for returning document to keep in memory, no save                   Path string for writing in given path</p> <code>False</code> <p>Copy sequence reversed if from_page &gt; to_page.</p> <p>Returns:</p> Name Type Description <code>_type_</code> <code>Document | None</code> <p>description</p>"},{"location":"code-reference/pdf_utils/#aker_utilities.pdf_utils.PDFEditor.merge","title":"merge  <code>staticmethod</code>","text":"<pre><code>merge(docpaths, save_as=None, recursive_folder_scan=False)\n</code></pre> <p>Merge given list of individual pdf paths or all documents in given folder path recursively or not recursively based on recursive_folder_scan argument.</p> <p>Parameters:</p> Name Type Description Default <code>docpaths</code> <code>str | list[str]</code> <p>Could be either folder</p> required <code>save_as</code> <code>str | None</code> <p>Saves into folder of first docpath or given                             string of folder path</p> <code>None</code> <code>recursive</code> <code>bool</code> <p>description. Defaults to False.</p> required"},{"location":"code-reference/pdf_utils/#aker_utilities.pdf_utils.PDFEditor.rotate","title":"rotate","text":"<pre><code>rotate(pages=None, step=1, save_as=None)\n</code></pre> <p>Rotate given pages in steps of 90 degrees, negative sign for counter clockwise</p> <p>Parameters:</p> Name Type Description Default <code>pages</code> <code>list[int]</code> <p>Page numbers as in pdf. (1-index) Defaults to all</p> <code>None</code> <code>step</code> <code>int</code> <p>description. Defaults to 1.</p> <code>1</code> <code>save_as</code> <code>str</code> <p>description. Defaults to None.</p> <code>None</code>"},{"location":"code-reference/pdf_utils/#aker_utilities.pdf_utils.PDFEditor.split","title":"split","text":"<pre><code>split(page, save_as=None)\n</code></pre> <p>Split given pdf by the given page number as in pdf. Keeps toc in splitted document</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>int</code> <p>Page number (incl) to split (1-index) as shown in pdf,</p> required <code>save_as</code> <code>str</code> <p>Custom fullpath. Defaults to same path as pdf</p> <code>None</code>"},{"location":"code-reference/regex_utils/","title":"Regex utils","text":""},{"location":"code-reference/regex_utils/#aker_utilities.regex_utils.convert_to_snake_case","title":"convert_to_snake_case","text":"<pre><code>convert_to_snake_case(s)\n</code></pre> <p>Convert mixed case string to snake case.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>Text to convert to snake case.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code>"},{"location":"code-reference/regex_utils/#aker_utilities.regex_utils.get_dbname_by_regex","title":"get_dbname_by_regex","text":"<pre><code>get_dbname_by_regex(table_name)\n</code></pre> <p>Get cleaned dbname out of datawarehouse dbname as shown below.</p> Example <pre><code>&gt; get_dbname_by_regex(\"Synergi_3_1_ActionCategory\")\nsynergi\n\n&gt; get_dbname_by_regex(\"SAP_SuccessFactors_1_1_WorkOrder\")\nsap_successfactors\n\n&gt; get_dbname_by_regex(\"cleanProd_temp\")\ncleanprod_temp\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>description</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: Silently failes by returning None if no match found.</p>"},{"location":"code-reference/regex_utils/#aker_utilities.regex_utils.get_table_name_by_regex","title":"get_table_name_by_regex","text":"<pre><code>get_table_name_by_regex(table_name)\n</code></pre> <p>Get cleaned table name out of datawarehouse db table names as shown below.</p> Example <pre><code>&gt; get_table_name_by_regex(\"Synergi_3_1_ActionCategory\")\nActionCategory\n\n&gt; get_table_name_by_regex(\"SAP_SuccessFactors_1_1_WorkOrder\")\nWorkOrder\n\n&gt; get_table_name_by_regex(\"cleanProd_temp\")\ncleanProd_temp\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>description</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>description</p>"},{"location":"code-reference/regex_utils/#aker_utilities.regex_utils.regex_compile_match","title":"regex_compile_match","text":"<pre><code>regex_compile_match(pattern, string)\n</code></pre> <p>To test regex compilation patterns and expected results in parametrized pytest func</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>_type_</code> <p>description</p> required <p>Returns:</p> Name Type Description <code>_type_</code> <code>bool</code> <p>description</p>"},{"location":"code-reference/regex_utils/#aker_utilities.regex_utils.validate_email","title":"validate_email","text":"<pre><code>validate_email(email)\n</code></pre> <p>Ensure email is valid using regex pattern matching.</p> <p>Parameters:</p> Name Type Description Default <code>email</code> <code>str</code> <p>Email address to validate.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If email is not valid.</p> <p>Returns:</p> Type Description <code>bool | ValueError</code> <p>bool | ValueError: True if email is valid, else raise ValueError.</p>"},{"location":"code-reference/sql_data_model/","title":"Sql data model","text":""},{"location":"code-reference/sql_data_model/#aker_utilities.sql_data_model.SQLDataModel","title":"SQLDataModel","text":"<pre><code>SQLDataModel(provider, database, server=None, instance=None, port=None, is_azure=False, is_trusted_connection='yes', options=None)\n</code></pre> <p>Creates an SQL alchemy connection with expanded API.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>Literal['sqlite', 'postgresql', 'mssql']</code> <p>Database provider</p> required <code>database</code> <code>str</code> <p>Database name or filepath for sqlite</p> required <code>server</code> <code>str | None</code> <p>ie. \"DWH-SQL-PROD' Defaults to None for sqlite.</p> <code>None</code> <code>instance</code> <code>str | None</code> <p>ie. 'XPERTBI' Defaults to None.</p> <code>None</code> <code>port</code> <code>int | None</code> <p>ie. 666. Defaults to None.</p> <code>None</code> <code>is_azure</code> <code>bool</code> <p>Defaults to False.</p> <code>False</code> <code>is_trusted_connection</code> <code>Literal['yes', 'no', 'test']</code> <p>Def. to \"yes\"</p> <code>'yes'</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>description</p> <code>ValueError</code> <p>Server and instance must be provided if provider is not sqlite.</p>"},{"location":"code-reference/sql_data_model/#aker_utilities.sql_data_model.SQLDataModel.execute_query","title":"execute_query","text":"<pre><code>execute_query(sql)\n</code></pre> <p>Executes given sql query on the database and returns the result as a list of dict</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>description</p> required <p>Returns:</p> Type Description <code>list[dict[str, str]]</code> <p>list[dict[str, str]]: description</p>"},{"location":"code-reference/sql_data_model/#aker_utilities.sql_data_model.SQLDataModel.generate_orm_model","title":"generate_orm_model","text":"<pre><code>generate_orm_model(gen_class, table_schemas=None, table_regex=None, tables_to_ignore=None, output_folder=None)\n</code></pre> <p>Generates ORM model for given database connection and exports to a file named with database name if output folder is provided.</p> <p>Example:</p> <p><pre><code>prod_bi_detnor_ods_synergi = SqlDataModel(\n    provider=\"mssql\",\n    server=\"DWH-SQL-PROD\",\n    instance=\"XPERTBI\",\n    port=None,\n    database=\"BI_DetNor_ODS\",\n    is_azure=False,\n    is_trusted_connection=\"Yes\",\n    options=None,\n)\n\nprod_bi_detnor_ods_synergi.generate_orm_model(\n    gen_class=\"table\",\n    output_folder=Path(\"TableGen_DWHPROD_BIDetNorODS_SynergiStatus.py\")\n)\n</code></pre> Args:     gen_class (Literal[\"table\", \"declarative\", \"dataclass\"]): description     table_schemas (list[str] | None, optional): description. Defaults to None.     table_regex (str | None, optional): description. Defaults to None.     tables_to_ignore (list[str] | None, optional): description. Defaults to None     output_folder (os.PathLike | None, optional): description. Defaults to None.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>description</p>"},{"location":"code-reference/sql_data_model/#aker_utilities.sql_data_model.SQLDataModel.generate_sql_ddl","title":"generate_sql_ddl","text":"<pre><code>generate_sql_ddl(table_schemas=None, table_regex=None, tables_to_ignore=None, keep_brackets=False, output_folder=None)\n</code></pre> <p>Generates ORM model for given database connection and exports to a file named with database name if output folder is provided.</p> <p>Example: <pre><code>prod_bi_detnor_ods_synergi = DbModel(\n    provider=\"mssql\",\n    server=\"DWH-SQL-PROD\",\n    instance=\"XPERTBI\",\n    port=None,\n    database=\"BI_DetNor_ODS\",\n    is_azure=False,\n    is_trusted_connection=\"Yes\",\n    options=None,\n    table_schemas=None,\n    table_regex=\"^Synergi_3_1_Status((?!Transaction_XBI).)*$\",\n    tables_to_ignore=None\n)\n\nprod_bi_detnor_ods_synergi.generate_ddl(\n    outputfile=\"DDL-DWH-PROD-BI_DetNorODS-Synergi_Status.sql\"\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>table_schemas</code> <code>list[str] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>table_regex</code> <code>str | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>tables_to_ignore</code> <code>list[str] | None</code> <p>description. Defaults to None</p> <code>None</code> <code>keep_brackets</code> <code>bool</code> <p>description. Defaults to False.</p> <code>False</code> <code>export</code> <code>bool</code> <p>description. Defaults to False.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>description</p>"},{"location":"code-reference/sql_data_model/#aker_utilities.sql_data_model.SQLDataModel.get_table_column_properties","title":"get_table_column_properties","text":"<pre><code>get_table_column_properties(table_schemas=None, table_regex=None, tables_to_ignore=None)\n</code></pre> <p>Returns dictionary with table name as key and list of dictionary of column properties as values</p> <p>Parameters:</p> Name Type Description Default <code>table_schemas</code> <code>list[str] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>table_regex</code> <code>str | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>tables_to_ignore</code> <code>list[str] | None</code> <p>description. Defaults to None</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, list[dict[str, str]]]</code> <p>dict[str, list[dict[str, str]]]: description</p>"},{"location":"code-reference/sql_data_model/#aker_utilities.sql_data_model.SQLDataModel.get_tables","title":"get_tables","text":"<pre><code>get_tables(table_schemas=None, table_regex=None, tables_to_ignore=None)\n</code></pre> <p>Get list of tables for db connection as a tuple of schema and table</p> <p>Parameters:</p> Name Type Description Default <code>table_schemas</code> <code>list[str] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>table_regex</code> <code>str | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>tables_to_ignore</code> <code>list[str] | None</code> <p>description. Defaults to None</p> <code>None</code> <p>Returns:</p> Type Description <code>list[tuple[str, str]]</code> <p>list[tuple[str, str]]: description</p>"},{"location":"code-reference/sql_utils/","title":"Sql utils","text":""},{"location":"code-reference/sql_utils/#aker_utilities.sql_utils.set_connection_string","title":"set_connection_string","text":"<pre><code>set_connection_string(server, instance, port, database, is_azure, is_trusted_connection)\n</code></pre> <p>Creates connection string for given database</p>"},{"location":"code-reference/sql_utils/#aker_utilities.sql_utils.set_connection_string--pyodbc-for-dwh-db","title":"PYODBC for DWH DB","text":"<p>conn = set_connection_string(     server=\"DWH-SQL-PROD\",     instance=\"XPERTBI\",     database=\"DM_MasterData\",     port=None,     is_azure=False,     is_trusted_connection=\"yes\" )</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>str</code> <p>description</p> required <code>instance</code> <code>str</code> <p>description</p> required <code>port</code> <code>int</code> <p>description</p> required <code>database</code> <code>str</code> <p>description</p> required <code>is_azure</code> <code>bool</code> <p>description</p> required <code>is_trusted_connection</code> <code>str</code> <p>description</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>description</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>description</p>"},{"location":"project/PPFG/","title":"Future DataOps PPFG Data Product","text":"<p>\u00abRequirements mapping\u00bb</p> <p>Digital Well Planning / Pore Pressure and Fracture Gradient (PPFG) curves for use in Well Engineering, in the planning phase / DWP</p> <p>Template usage</p> <p>This template is used as a template for requirements specifications. Not all chapters need to be filled out, but the template should be seen as a checklist. We only fill out what is necessary and relevant.</p> Metadata Author: Henrik N\u00e6sgaard / Data Product Support TeamDate: 2024.09.XXVersion: Draft 1.1"},{"location":"project/PPFG/#document-control-mandatory","title":"Document Control (Mandatory)","text":""},{"location":"project/PPFG/#template-history","title":"Template History","text":"Who Revision Date Comment Alexander Tharaldsen Final 1.0 29.05.2024 Template created"},{"location":"project/PPFG/#document-history","title":"Document History","text":"Who Revision Date Comment Henrik N\u00e6sgaard Draft 0.0 10.06.2024 Draft Version 0.0 Henrik N\u00e6sgaard Draft 1.0 07.07.2024 Draft Version 1.0 + attempted to adjust for new Template (Final 1.2) Frode Efteland Draft 1.1 12.11.2024 Draft version 1.1. Comments and changes by the Data Product Support Team."},{"location":"project/PPFG/#action-list","title":"Action List","text":"Who Task Comment"},{"location":"project/PPFG/#references","title":"References","text":"Description Link PPFG to EDM Solutioning by DWP team https://miro.com/app/board/uXjVPlZxm5E=/ Data Product Canvases for PPFG as used when applying Guideline to practical use case https://miro.com/app/board/uXjVNeiYapo=/ TR for PPFG Prediction 51-001183 Guideline for pore-pressure fracture gradient prediction (PPFG) and well stability (WBS) TR for PPFG Forecasting 51-000566 Technical Requirements for Pore, Fracture and Collapse Pressure Forecasting PPFG Review template 51-001619 PPFG review template Latest IOGP Report IOGP Report 608 - Recommended practice for pore pressure and fracture gradient analysis for well design \u2013 construction, intervention, and abandonment"},{"location":"project/PPFG/#purpose-of-the-project-mandatory","title":"Purpose of the Project (Mandatory)","text":""},{"location":"project/PPFG/#project-goals","title":"Project Goals","text":""},{"location":"project/PPFG/#project-goals_1","title":"Project goal(s):","text":"<ul> <li> <p>Expose PPFG curves to WellMate (Necessary delivery for the Drilling   Program for Yggdrasil by August 2024)</p> </li> <li> <p>Expose PPFG curves to DWP (Drilling Engineers in ABP for Well   Engineering including Casing Design, Trajectory, Operational   Parameters, BHA, Bit, Drilling Fluid, Cementing, Kick Tolerance,   Integrity/Barrier tests, Detailed Drilling Procedures. In planning   phase, this is represented by Digital Well Planning (DWP). ABP   Christian Jacobsen is PO.)</p> </li> <li> <p>Expose PPFG curves to DWOS (Needed data to allow for any work to be   performed in DWOS from training to engineering simulations; the bread   and butter of the DWOS. Full application listing for engineering:   WellPlanner, DrillExpect, DrillScene, WellPlan, DFG RTE.)</p> </li> <li> <p>Expose PPFG curves for the CWP session (for multidisciplinary   visualisation)</p> </li> <li> <p>Expose PPFG curves to DWA (Necessary input for the Advisory systems   for automation drilling)</p> </li> <li> <p>Expose PPFG curves to Sitecom WellAdvisor</p> </li> <li> <p>Expose PPFG curves to central repository for tracking of   decision-making and for general availability of Data Products in   AkerBP. This is proposed solved in FDP and ADME, but the   benefit/cost/timeline of this sustainable integration, needs to be   evaluated against quicker point-to-point implementations.</p> </li> <li> <p>Expose PPFG curves to Gravitas</p> </li> </ul>"},{"location":"project/PPFG/#timeline","title":"Timeline:","text":"<p>Successfully launching this project would generate significant value and cost savings, particularly if it is completed before Q3 2024 (1<sup>st</sup> August 2024), when the Yggdrasil operation is scheduled to commence.</p>"},{"location":"project/PPFG/#background","title":"Background","text":"<p>There are several initiatives that need the PPFG data, but the one that has the highest priority (from the point of view of Henrik N\u00e6sgaard) is WellMate (the Digital Drilling Program). This initiative needs the PPFG by August 2024. The importance of the WellMate delivery cannot be restated in fullness here, but can be attempted summarized by underlining that the Yggdrasil Team has stated that they will not be able to create Drilling Programs in time due to insufficient resources, unless a digital solution is presented that can automate some of the workflow. The transfer of subsurface data (including PPFG) is in scope for this.</p> <p>Since automation by August 2024 was not reached, the WellMate team has bought some time by gaining priority with the DWP team for a manual Excel import of the required curves for Well Engineering according to BMS. This solution is only temporary, a more automated dataflow MUST be instated. The required curves are listed below.</p> <ul> <li> <p>Pore Pressure (PP) P5-10</p> </li> <li> <p>Pore Pressure Expected</p> </li> <li> <p>Pore Pressure High P95-99</p> </li> <li> <p>Fracture Gradient (FG) P5-10</p> </li> <li> <p>Fracture Gradient Expected</p> </li> <li> <p>Fracture Gradient P95-99</p> </li> <li> <p>Minimum Horizontal Stress (Shmin) P5-10</p> </li> <li> <p>Minimum Horizontal Stress (Shmin) Expected</p> </li> <li> <p>Minimum Horizontal Stress (Shmin) P95-99</p> </li> <li> <p>Collapse Gradient (CG) Expected</p> </li> <li> <p>Overburden Gradient (OBG) Expected</p> </li> </ul> <p>Below is presented a typical PPFG plot as completed in Gravitas for the Drilling Program, for visualisation (format: pdf snippet).</p> <p></p> <p>It is also the belief of the WellMate team that pulling the PPFG data from EDM/DWP makes sense from the perspective that the Drilling Program should summarize the data that was used for the Well Design (which is carried out in EDM/DWP). Combined with a general desire to expose data to all, the team hopes that a solution for exposing data to EDM can be extended to any other subscriber.</p> <p>From an operational safety perspective, it is an absolute necessity to expose and govern this data as input for Well Engineering. A few examples (among thousands) for usage areas are casing loads, operational window during drilling, selection of drilling tools, trajectory optimization, etc... These data form the decision input package for the well design and operational planning. If erroneous, it could lead to operational events such as kicks and losses. Conversely, better data quality can green-light less over-designed wells, reducing cost. The correct implementation strategy will result in mitigating barriers against poor engineering, reduce risk, duplication of data resulting in ungoverned data, etc...</p>"},{"location":"project/PPFG/#business-case","title":"Business case","text":"<p>Although not defined as \"safety critical\", PPFG should be defined as such, as the highest impact outcome could be a blowout with loss of life. One level lower of impact, we find the cost implications of long operational delays due to losses, kicks and wellbore instability. On a lower level of impact, we find costs incurred through poorly coordinated well designs. Reduction in downhole operational events of 3% across assets over 2 years. Reduction of capex cost of well design reduced by 5% across assets over 3 years.</p>"},{"location":"project/PPFG/#stakeholders-optional","title":"Stakeholders (Optional)","text":"<p>Main stakeholder</p> <p>[NAME] the &lt;CHARACTERISTIC&gt;</p> <p>Drilling Engineer Henrik, Member of Yggdrasil Well Engineering Team.</p> <p>36 years young and well-built (athletic). Many more positives too list, but space is limited. Extremely humble.</p> <p>\u201cI have done well designs several times before and know of the tedious process\u201d</p> <p>\u201cI value quick iterations for more sensitivities for more accurate engineering. I despise governance issues and spending unnecessary time on coordination\u201d</p> <p>Routines (to pinpoint challenges)</p> <p>[Henrik] needs to</p> <ul> <li><p>Update the trajectory in EDM as a result of T&amp;D engineering</p></li> <li><p>Inform Operational Geologist that I have made changes and convince that time is well spent, gathering a new set of input data</p></li> <li><p>X days later the new data is prepared. Process and lineage unknown.</p></li> <li><p>I am given a link to Sharepoint or sent the PPFG on email</p></li> <li><p>I receive different PPFG for different Assets/employees. I call OpsGeo to ask which data from the Excel sheet that I should use</p></li> <li><p>I import a maximum of 2 curves into DWP. It is an annoying process that is prone to failure (maximum ~450 points, randomly unstable, poor UX)</p></li> <li><p>I complete my engineering based on 2 curves selected. For more sensitives, I need to repeat the work done in DWP.</p></li> <li><p>Drilling Program is ready to be completed, I ask the OpsGeo for a low-quality, non-interactive snippet from Gravitas that the offshore team will use to make decisions.</p></li> </ul> <p>Wishes he has</p> <p>The wishes should be prioritized:</p> <ol> <li><p>Remove redundant manual work. Update PPFG only one place (duplication)</p></li> <li><p>Make sure I am using the correct PPFG (governance)</p></li> <li><p>Ensure the correct PPFG is exposed to interested initiatives (exposure)</p></li> <li><p>Understand the maturity of the data, understand where it comes from, understand how I should use it (lineage)</p></li> </ol> <p>Goals</p> <ul> <li><p>Conduct Well Engineering tasks to increase safety and robustness of design</p></li> <li><p>Reduce cost to ALARP</p></li> <li><p>Complete necessary deliverables such as Drilling Programs</p></li> </ul> <p>Tools</p> <p>Everyday, Henrik uses \u2026.</p> <ul> <li><p>DWP/EDM</p></li> <li><p>Powerpoint, Excel, Sharepoint</p></li> </ul> <p>Sometimes, [he] uses\u2026..</p> <ul> <li><p>N/A</p></li> </ul> <p>Person does not need:</p> <ul> <li><p>Unproductive, brainless work</p></li> <li><p>Time-wasters</p></li> </ul> <p>Other key Stakeholders</p> stakeholder Needs <p>Customer (Asset/BU/Dept)</p> <p>Drilling Engineers</p> <p>Completion Engineers</p> <p>Lead Engineers</p> <p>Asset Leads</p> <p>Needs the Well Design data.</p> <p>Needs the Well Design data.</p> <p>Accountable for Well Design (cost/HSE, etc..)</p> <p>Accountable for Well Design (cost/HSE, etc..)</p> Customer's Customer (VSM/User) <p>Other Stakeholders</p> <p>Rock Mechanics</p> <p>Subsurface</p> <p>EXPRES</p> <p>Geology</p> <p>Geophysicist</p> <p>Owner of the Data</p> <p>Provides input to Data</p> <p>Provides input to Data</p> <p>Provides input to Data</p> <p>Provides input to Data</p> <p>Personas</p> <p>PPFG user</p> <p>PPFG provider</p> <p>Subscriber/consumer</p> <p>Creator/owner</p>"},{"location":"project/PPFG/#terms-and-definitions-mandatory","title":"Terms and Definitions (Mandatory)","text":"Term description ABP AkerBP (Company) ALARP As Low As Reasonably Practicable API Application Programmable Interface AS RUN Status when completed after Execution (aka Post-Drill or Actual) Backbone Database solution provided by Keystone BEST Terminology applied to subsurface curves for the \u201clatest and greatest\u201d BHA BottomHole Assembly (equipment) BMS Business Management System BOWK BlowOut and Well Kill (module in DWP) BU Business Unit (organisational department) CG Collapse Gradient (aka Wellbore Stability WBS curve) COMPASS Application for Directional Planning by Halliburton Csv File format CWP Collaborative Well Planning (process) DG Decision Gate (process milestone) DFG RTE Halliburton software for real-time Fluids/Hydraulics DSG DecisionSpace Geosciences application by Halliburton DSIS Data transfer solution (Halliburton systems) DWA DWA: Digital Well Automation, platform installed on-site for sharing of data and drilling advisory. DWOS DWOS: Drilling Well On Simulator, Simulator training and Engineering Simulations in the ABP OCC (Onshore Collaboration Center). DWP DWP: Digital Well Program, Halliburton software for Well Engineering. DWX Digital Well eXchange. Data transfer method by Halliburton (Kafka) EDM Engineering Data Model. Database for DWP. EXPRES SME EXPlorationREServoir Subject Matter Expert FA Friction Angle (Rock Mechanics curve) FDP Field Development Plan. Application for Asset Management by Halliburton Execution Project state for D&amp;W, equivalent to DG3-WG2, during drilling of the Well. HSE Health, Security and Environment ID Identifier IDOS IDOS: integrated Design of Service, Halliburton software for managing Service Company Engineering deliverables. IOGP International Association of Oil &amp; Gas Producers KIND Term used in OSDU Standard for the Schema name and version. LAS File format MSL Mean Sea Level. Often used as datum for Well Operations. OBG Overburden Gradient (Rock Mechanics curve) OpsGeo Operational Geologist (role) OSDU OSDU: data platform for Energy companies OW OpenWorks software by Halliburton Petrel Software by Schlumberger Petrel Studio Software by Schlumberger Planning Phase Period extending from pre-DG0 to DG3-WG2 (start of Execution). PO Project Owner (role) Post-Drill State after completion of relevant operational step in Execution (aka AS RUN or Actual) PostGres PostGreSQL Database system PPFG <p>PPFG (Pore Pressure and Fracture Gradient): Curves indicating the pressure in the formation pores and the strength of the rock. In practice, these are used as minimum and maximum energy limits to apply to the hole being drilled.</p> <p>OSDU PPFG def: _Pore Pressure and Fracture (Pressure) Gradient (PPFG) data describes the predicted (Pre-drill) and actual (Post-drill) depth-related variations in overburden stress, pore pressure, fracture pressure and minimum principal stress within a wellbore and conveys the range of uncertainty in these values given various plausible geological scenarios. PPFG predictions are fundamental inputs for well design and construction and estimates of pore and fracture pressure are typically provided to the well planning and execution teams.</p> Pre-Drill State prior to operational Execution, hence estimations and predictions from the Planning Phase. PVT Pressure, Volume and Temperature. Dataset giving characteristics of an object under variations of these three parameters. QAQC Quality Control and Quality Assurance Record A populated schema RM Rock Mechanics (AkerBP Business Unit. Owners of PPFG) ROCQ Company providing data transfer solutions Schema Structure of datafile. Not a Record. SCWA SCWA: SiteCom WellAdvisor Shmin Minimum Horizontal Stress (Rock Mechanics curve) Snapshot Captured copy of record at the time of snapshot trigger Stresscheck Application for casing design by Halliburton TR Technical Requirement Trajectory Loop Abstraction of the non-existing digital process of looping the trajectory between Subsurface and Drilling BUs, while attaching updated datasets (including PPFG) TVD Terminal Vertical Depth (or Total/True Vertical Depth) UCS Unconfined Compressive Stress (Rock Mechanics curve) UX User eXperience (design term) WBS Wellbore Stability (aka Collapse Gradient. Rock Mechanics curve) WellMate Digital Drilling Program: Software that pulls information from different source to auto-generate the Drilling Program. WITSML Wellsite Information Transfer Standard Markup Language (data exchange standard) WKS WKS: Well Known Schemas; current approved schemas of the OSDU standard"},{"location":"project/PPFG/#relevant-facts-and-assumptions-optional","title":"Relevant Facts and Assumptions (Optional)","text":""},{"location":"project/PPFG/#relevant-facts","title":"Relevant Facts","text":""},{"location":"project/PPFG/#assumptions","title":"Assumptions","text":"Assumption Project Phase Status Replaced by Requirement PPFG needs to be locked to a Well Design_ID (trajectory). N/A N/A N/A Can come through CWP session or from Petrel. The mechanism is not really important to the Drilling Team; as long as it is defined and supported by the Subsurface team. N/A N/A N/A Will be governed in FDP, stored on OSDU. DWX as transfer mechanism. N/A N/A N/A Keep only at PPFG for now (do not include Picks and other Geomechanical curves such as UCS and FA). N/A N/A N/A The assumption must be made that the updates are pushed from Petrel Studio (for simplicity). N/A N/A N/A The assumption must be made that the updated PPFG curves are stored in PostGres when the destination is DWP/EDM (to not disrupt EDM engineering or crash EDM). N/A N/A N/A If we build a system with no notification system, there should be some human-relatable attribute to the flow such that we can verify through something simple like the name. (to build trust. Hence retain the name across applications.) N/A N/A N/A A microservice will be needed for coordinate system Conversion, when sharing across applications. N/A N/A N/A Collapse Gradient is calculated based on microservice that uses PP as input. N/A N/A N/A Petrel understands Well/Wellbore hierarchy. N/A N/A N/A Exchange of data is done at Wellbore level, not Well level. N/A N/A N/A Explicitly, this request is for planning PPFG only. Not execution or AS RUN PPFG. N/A N/A N/A Petrel Studio will not propose new trajectories. N/A N/A N/A DWP will not propose new PPFG, WBS or Formations. N/A N/A N/A It is NOT possible to retain the trajectory_ID upon copying designs in DWP (EDM). N/A N/A N/A It is very challenging to retain Design_ID (or the ID of the EDM trajectory: definitive_survey_header_id) across the trajectory loop Petrel Studio-&gt;CWP-&gt;EDM-&gt;Petrel Studio. N/A N/A N/A Should strive to have an open API for ingestion of PPFG for DWP (better for R&amp;D -&gt; more urgency and buy-in). Avoid ABP-customized solution! N/A N/A N/A Petrel Studio user can observe desynchronization against DWP trajectories. N/A N/A N/A There is no way for the Petrel user to know if the trajectory has had minor changes since last pull. This is NOT covered by ROCQ governance. N/A N/A N/A From an Operational standpoint, it would be advisable to have the curves separated by Overburden/Reservoir. The process to create the Reservoir curves is usually more complex due to its criticality (this sentence to be confirmed/reviewed by EXPRES SME). N/A N/A N/A"},{"location":"project/PPFG/#scope-of-work-optional","title":"Scope of Work (Optional)","text":""},{"location":"project/PPFG/#current-situation","title":"Current Situation","text":"<ul> <li> <p>Excel import in EDM</p> </li> <li> <p>Limited to 2 curves</p> </li> <li> <p>No governance</p> </li> </ul>"},{"location":"project/PPFG/#work-context","title":"Work Context","text":"<p>In context with Well Design BMS.</p>"},{"location":"project/PPFG/#mvp-scope","title":"MVP scope","text":"<ul> <li> <p>Deliver Yggdrasil PPFG data from Delfi in a consumable format for DWP.</p> </li> <li> <p>The format should be OSDU ready.</p> </li> </ul>"},{"location":"project/PPFG/#functional-requirements-mandatory","title":"Functional Requirements (Mandatory)","text":"<p>| **Requirement ID** | **Requirement Name** | **Description** | **Priority** | **Source** | **Acceptance Criteria** | **Use Case/Scenario** | **Actors/User Roles** | **Dependencies** | **Non-functional Attributes** | **Assumptions** | **Comments/Notes** |</p> <p>Guidelines for Data Products, apply the following best practices:</p> <ol> <li> <p>Identify and prioritize candidates for the Data Product approach, to     ensure best possible use of resources and reduce risk.</p> </li> <li> <p>Design a new Data Product as a minimum viable product to ensure     early usage and value outtake.</p> </li> <li> <p>Ensure that the data is produced according to the Data Product     design, and verify and validate its consistency before sharing.</p> </li> <li> <p>Before making the Data Product ensure that it is fit for the purpose     you want to apply it for.</p> </li> </ol>"},{"location":"project/PPFG/#schema-and-semantics-lineage","title":"Schema and Semantics, Lineage","text":"<ul> <li> <p>See chapter on Schema in \u201cIdeas for solutions\u201d (Proposed schema)</p> </li> <li> <p>PPFG includes a reference to a trajectory, Wellbore and Well</p> </li> <li> <p>PPFG schema should be as OSDU WKS compliant as possible.</p> </li> <li> <p>PPFG schema should detail which model was used, author, date created   and typical metadata (lineage).</p> </li> <li> <p>PPFG should be made on communicated assumptions (assumptions described   by Owner in the record or defined as part of Data Product description   in Catalogue or addressed by standardized input workflow or otherwise   communicated to consumers)</p> </li> <li> <p>From an Operational standpoint, it would be necessary to have the   curves by wellbore.</p> </li> <li> <p>From an Operational standpoint, it would be advisable to have the   curves separated by Overburden/Reservoir. The process to create the   Reservoir curves is usually more complex due to its criticality (this   sentence to be confirmed/reviewed by EXPRES SME). For the Final Well   Report Section A - Geology we usually log the Overburden separately   from the Reservoir (including for PPFG reporting).</p> </li> <li> <p>\u201cBEST-curves\u201d is the terminology used in ABP for the best curves   available at any point in time. Correct terminology and classification   to be done in schema.</p> </li> <li> <p>The PPFG curves should ideally be made available as a range (min, exp,   max or their P10, P50 and P90 equivalents).</p> </li> <li> <p>\u201cMin/exp/max\u201d curve is a \u201cDrilling Engineer term\u201d. Subsurface team   relates differently to the uncertainty of geomechanical curves.   Correct terminology and classification to be done in schema.</p> </li> <li> <p>Datum assignment. As per the schema. Probably the PPFG (from OpenWorks   or Petrel) needs to be on a standard datum, where MSL is the system   default in EDM.</p> </li> <li> <p>The \u2018PPFG forecast\u2019 shall capture the expected pore and fracture   gradient and its range of uncertainty as function of depth. The output   from Petrel/Delfi, shall follow the technical requirement specified in   D2 51-000566. Additional technical detail can be found in document   51-001183: Guideline for porepressure fracture gradient prediction   (PPFG) and well stability (WBS)</p> </li> <li> <p>Need to update this document if it changes ExpRes workflow. See     section 8.1 Digital Archives.</p> </li> </ul>"},{"location":"project/PPFG/#architecture-validations","title":"Architecture, validations","text":"<ul> <li> <p>PPFG validation microservice should validate the schema wrt to   Business Rules</p> </li> <li> <p>PPFG transactions should be logged in FDP for in-depth analytics of   the Well Design process, store decisions and to take up dormant   projects at a later date.</p> </li> <li> <p>PPFG should not be copied multiple times into several consumer   applications and use cases, using conventional means/manual work</p> </li> <li> <p>PPFG should be exposed to other initiatives than DWP (DWA, IDOS, CWP,   DWOS, WellMate, SCWA).</p> </li> <li> <p>DSIS query towards OW needs to have a delimiter on Design_ID.   Trajectory_ID limitation is also nice-to-have. The assumption is that   a \"significant enough change\" triggers a copy of the EDM design with a   renaming of the design (entirely manual process). The name change   would be something along the lines of \"30/11-F-2 A_Rev B0\" to   \"30/11-F-2 A_Rev C0\". The copy of the design would generate new IDs   for the design and all data attached to it (including   def_survey_header_ID). This means the lineage and versioning of the   trajectory and design (and everything else) is not maintained   adequately regardless. This in turn, means that it is not required to   send over both Design_ID and Trajectory_ID, where Design_ID is   sufficient. We can do it from an additional QAQC standpoint and with   the future in mind. Question: can we retain these IDs throughout the   loop EDM-&gt;Petrel Studio-&gt;OW-&gt;Petrel Studio?</p> </li> </ul>"},{"location":"project/PPFG/#governance-and-versioning-ownership","title":"Governance and versioning, Ownership","text":"<ul> <li> <p>Digital logging and storage of PPFG should be iterative and   Decision-Gate-Less</p> </li> <li> <p>Versioning of the PPFG is imperative</p> </li> <li> <p>Solution needs to map out whether there is a requirement from Drilling   Engineers on whether to not trigger a PPFG update. In other words, is   it worthwhile to define \u201ca big enough change\u201d to require a new PPFG   iteration? Do they want a choice for: \u201cA change in the trajectory has   been detected, do you want to trigger an iteration now?\u201d. What does   \u201cupdated/changed mean\u201d? Assume it is \u201ca big enough change\u201d that would   trigger a Design copy in EDM and a new Design with new trajectory   name.</p> </li> <li> <p>Unless we snapshot the Trajectory on export from DWP, how can we   rightfully import PPFG based on trajectory_ID? The trajectory may have   been changed. Only barrier is a forced copy of design and store in EDM   for every significant change (\"significant change is when you should   request PPFG update; minor change should not trigger new loop and is   hence acceptable). Every major change should then force a new design   with a new trajectory (and trajectory_ID). Every request to Petrel   should then be a consequence of a major change (and new design,   trajectory and trajectory_ID in EDM). This means that trajectory_ID is   ONLY useful for the DWP PostGres import locked to Trajectory_ID. This   workflow is very demanding for the users and riddled with pitfalls.</p> </li> <li> <p>The solution needs to be reviewed by Rock Mechanics. \u201cWe do need to   assure the data owners in rock Mech that the data used is \"theirs\" -   this is critical to get trust.\u201d. \u201cWe need a clear path for verified,   recognizable data from the source model (petrel) to the targets   needing it.\u201d. \u201cThe Rock Mech engineers are not your typical petrel   user - we will need to make sure they are on board.\u201d We need the   proposed workflow to be verified by Rock Mechanics team</p> </li> <li> <p>What are the issues we are aware off?</p> </li> <li> <p>What is their workflow?</p> </li> <li> <p>Address these issues in our proposal before engaging the Rock     Mechanics Team</p> </li> <li> <p>The solutioning of the PPFG data exposure has shown to require   standardization in the workflow within Petrel to ensure that we get   consistent input. This is being worked on by Pete Heavey.</p> </li> </ul> <p></p>"},{"location":"project/PPFG/#security","title":"Security","text":"<ul> <li> <p>Access control should be managed</p> </li> <li> <p>A notification system is necessary due to potential impact of   erroneous information on Well Design.</p> </li> </ul>"},{"location":"project/PPFG/#related-initiatives-dwp","title":"Related Initiatives: DWP","text":"<ul> <li> <p>See Chapter \u201cRelated Initiatives\u201d in Ideas For Solution for   dependencies (Related initiatives).</p> </li> <li> <p>Detection/Notification of desynchronization. DWP User to be notified   that a change has happened on trajectory and that the geological data   might be outdated, and then have the ability to request new data. OR   in the perfect world with automation, as soon as a change has occurred   user automatically passes the trajectory to get data updated. Note: If   user duplicates a full design, and modifies PPFG, DWP will recalculate   everything based on this change using the old data, this means there   can be a risk at users do not update the data.</p> </li> <li> <p>First version: Just a notification in DWP. Could be a text string on     the component itself, \u201cHave you updated your trajectory in Compass?     Do you need new geo data? Click here\u201d</p> </li> <li> <p>Trigger for new trajectory loop. Is it required to have a trigger for   the Drilling Engineer to ask for a PPFG update or is the current   Petrel Governance tool sufficient?</p> </li> <li> <p>We need a PPFG request button in DWP. First version, when the user     thinks it is a big enough change to trigger a new data request from     subsurface. Assumed any change is resulting in a new design in EDM     as per process in EDM to day (minor or major change, a new design is     always made)</p> </li> <li> <p>Ability to request update on data package if a trajectory has been     changed/updated. Challenges: is this a button outside of DWP?</p> </li> <li> <p>Lock DWP Design while trajectory loop is in progress/updating. When   new PPFG data is requested, how do we stop engineers from modifying   the trajectory in the middle of waiting for the updated data?   Especially in a semi-automatic world. Can we lock design until data   has been made available and user \"imports\" this data to the design?   Consider linking this to \u201csignificant change\u201d, since a significant   change in trajectory is the only likely trigger.</p> </li> <li> <p>I need to understand why this would be needed. Also, pending     solutioning, there could be no \"requests for new PPFG\"; they are     just made as a result of monitoring for new trajectory updates (well     name and \"synced?\"-checkbox in Petrel).</p> </li> <li> <p>When new data is requested, how to we stop engineers to modify the     trajectory in the middle of waiting for the updated data? Especially     in a semi-automatic world. Can we lock design until data has been     made available and user \"imports\" this data to the design?</p> </li> <li> <p>Notification of new PPFG available. A DWP notification that the PPFG   data has been received would be nice. Who has sent the data should   also be available. One solution is to pull the date from OW, from   which the logical deduction is that we need a notification system in   DWP on new PPFG available in OW.</p> </li> <li> <p>Limiting import choices to correct trajectory only. When importing   PPFG data, DWP user should ONLY be able to select the relevant data.   The problem today is that users can go into ANY project and select ANY   well to import data, disregarding what design they are actually   working on. Can we re-use the interface we have (OpenWorks import),   but selection is filtered down to only relevant project in DSG? This   probably demands that the trajectory_ID is maintained. Other solutions   just slightly limit the number of wrong choices.</p> </li> <li> <p>Drilling Engineer requires to see the correct Design name (or some   other recognizable metric) when pulling the data from OW. How can that   person confirm visually (non-data technical), that what is being done   is correct?</p> </li> <li> <p>DWP requirement: The user must see Design_Name for import, even     though the requirement/filter is by the def_survey_header_ID     (\"trajectory_ID\").</p> </li> <li> <p>Prevent changes/alterations of subsurface data in DWP. Should not be   possible to edit PPFG data in DWP. If engineers want to make a   separate version for any sensitivities, they should do so by creating   a separate version, and this version should also be clearly marked as   \u201cnot original\u201d.</p> </li> <li> <p>Insufficient resources. Need to review solution in DWP team for   re-allocation of resources.</p> </li> <li> <p>Can we in DWP rename to \"Pull from External source\" for the Subsurface   Essentials? Even though the OpenWorks connection should be an easy   sell, it would be good to aim for something generic (both from DPW R&amp;D   Cores point of view and in view of our midterm goal of pulling from   OSDU).</p> </li> <li> <p>Strong dependency on naming convention, since other EDM copies of the   design are made for, among others, Sensitivity analysis</p> </li> <li> <p>There needs to be a common understanding that updates to trajectory     are made by adequately copying the EDM design and strictly adhering     to the naming convention. Then, the Petrel user needs to monitor, on     his own accord, for updates to trajectory. They will need to     identify by naming convention what necessitates a new PPFG. They     need to make the decision, on their own accord, to push a new     updated PPFG. The DWP user needs to, on his own accord, query the OW     database until the update appears and grab the update.</p> </li> <li> <p>Sensitivities: Casing design, Torque and drag, Hydraulics, Casing     setting depths, Reservoir lengths, BHA design, Fluid design</p> </li> <li> <p>We are allowing both pull from OW and manual upload, hence the loop   WILL be broken. This is acceptable as long as the manual upload is   seen as an informed decision to amend the data (short time during   execution). The problem is that this will not be captured by non-DWP   users; they will need to be comfortable with what is in OW. This is   tolerable in the short-term.</p> </li> </ul>"},{"location":"project/PPFG/#related-initiatives-wellmate-digital-drilling-program","title":"Related Initiatives: WellMate (Digital Drilling Program)","text":"<ul> <li> <p>See Chapter \u201cRelated Initiatives\u201d in Ideas For Solution for   dependencies (Related initiatives).</p> </li> <li> <p>Requirement from WellMate: The data needs to be exposed in a way that   works for InformatiQ.</p> </li> <li> <p>It is assumed that a solution that works for DWP and InformatiQ,     works for anyone. DWA, IDOS and DWOS are interested parties.</p> </li> <li> <p>(re-use of the limiters in the DSIS query)</p> </li> </ul>"},{"location":"project/PPFG/#related-initiatives-cwp","title":"Related Initiatives: CWP","text":"<ul> <li> <p>What is the connection with CWP FDP team? What is the delta to make   this workflow \"FDP-compliant\"? Does it make sense to pursue it?</p> </li> <li> <p>As far as I know the CWP FDP process transfers Targets and     Trajectory currently to EDM. How much work is it to expand this with     PPFG? If it takes 1 month more with this solution as it connects to     FDP, it is worth to do beyond any doubt.</p> </li> <li> <p>What is the difference in technical solution between transferring   trajectory and transferring PPFG, WBS and formations? In other words,   when we have a working trajectory loop, adding more data to the loop   is \"just\" a matter of expanding the exposure capability and building   out import workflows (this is up to each department).</p> </li> </ul>"},{"location":"project/PPFG/#maturityworkflows-and-lineage","title":"Maturity/workflows and Lineage","text":"<p>If not covered above, the requestor will answer these specific questions:</p> <ul> <li> <p>When do you need the Data Product with regards to maturity? (always   latest? Only the version signed in FDP for Decision Gate 3 pass?   Whenever Drilling Engineer makes an update in Execution?)</p> </li> <li> <p>Answer: There should be loop for the trajectory, where a defined     subsurface data package updates as a result of the trajectory     change. Hence, at any stage a change in trajectory is submitted.</p> </li> </ul> <ul> <li> <p>What do you need to know in order to trust the data you receive? (What   parameters do you need in the schema? Do you need to know what model   was used? Do you need to know the author? Do you know how to use the   range of data you are receiving; which in the dataset should you use?)</p> </li> <li> <p>Answer: Yes, I need all this information in the schema.</p> </li> </ul>"},{"location":"project/PPFG/#non-functional-requirements-mandatory","title":"Non-functional Requirements (Mandatory)","text":"<p>General design guidelines for non-functional requirements</p> <ul> <li> <p>Company Enterprise Reference Architecture, which includes</p> </li> <li> <p>Architecture &amp; Data principles</p> </li> <li> <p>Architecture &amp; Data standards/guidelines.</p> </li> <li> <p>Data management fundamentals</p> </li> <li> <p>Company data product framework</p> </li> <li> <p>BMS process for defining data products</p> </li> <li> <p>Guidelines described in DIGIpedia</p> </li> <li> <p>OSDU framework</p> </li> <li> <p>The data product will initially be developed with minimal data   governance, with governance gradually introduced in iterations until   it aligns with the business ambition.</p> </li> </ul> <ul> <li> <p>300 concurrent users</p> </li> <li> <p>Data Availability: internal</p> </li> <li> <p>Data Confidentiality: internal</p> </li> <li> <p>Data Integrity: High impact (blowout highest outcome)</p> </li> <li> <p>Performance: A transaction should take less than 5 minutes, but this   is expected to be substantially reduced over time with better   solutions and better computation abilities.</p> </li> <li> <p>Several concurrent transactions should be able to be handled</p> </li> <li> <p>Several concurrent contributors should be able to be handled.</p> </li> <li> <p>Governance requirements listed in Data Product Key Elements - AkerBP   DIGipedia</p> </li> </ul>"},{"location":"project/PPFG/#ideas-for-solutions-optional","title":"Ideas for Solutions (Optional)","text":""},{"location":"project/PPFG/#proposed-workflow","title":"Proposed workflow","text":"<ol> <li> <p>What we are currently prioritizing is to be able ingest PP-FG data     to DWP with the cross reference to what trajectory it belongs to     (was calculated based on).</p> </li> <li> <p>From an automation perspective the trigger for the re-calculation of     a PP-FG curve is that the user creates a new trajectory that he /she     says is valid.</p> </li> <li> <p>In a perfect world a valid PP-FG for this new Trajectory could be     requested from the relevant grid model and ingested into DWP.</p> </li> <li> <p>Always re-calculating the PP-FG based on the trigger of a new     trajectory defined as valid would remove the complexity of needing     to define or answer \u201chow much change requires a new PP-FG     Calculation.\u201d</p> </li> <li> <p>However, I\u2019m not saying this is the only option; if the source from     where the PP-FG comes from as described below is \u201canother design\u201d we     could use this, but we need to define a few things first:</p> <ol> <li> <p>\u201cHow much change of a trajectory requires a new PP-FG     Calculation\u201d?</p> </li> <li> <p>Is it okay that the user \u201capproves\u201d the use of this \u201cold\u201d PP-FG     on the new Trajectory?</p> </li> <li> <p>How do we track that this PP-FG curve is calculated from another     Trajectory?</p> </li> </ol> </li> </ol> <p></p>"},{"location":"project/PPFG/#related-initiatives","title":"Related initiatives","text":""},{"location":"project/PPFG/#related-initiative-wellmate-the-digital-drilling-program","title":"Related initiative: WellMate (the Digital Drilling Program)","text":"<p>Contact: Christian Elle (backend)</p> <p>WellMate is an application developed by InformatiQ to pull information from other sources to assemble the data needed for a Drilling Program (including trajectory, casing design, PPFG, etc..). The application features a \u201cpdf printer\u201d to export the data for conventional deliveries to authorities and partners.</p> <p>The project is backed by Yggdrasil asset that have stated that they are entirely dependent on a smart solution to quickly produce Drilling Programs (automation). This is because they do not have the manpower to write all the Programs (55+ ea wells). The delivery date of the solution is 1<sup>st</sup> August 2024.</p> <p>It is important that other subscribers such as InformatiQ, DWOS, DWA, IDOS are able to \u201cget\u201d the same PPFG package. Hence, the method for exposing the PPFG data must be considered in this context.</p>"},{"location":"project/PPFG/#related-initiative-current-dwp-workflow-for-importing-ppfg-2-ea-curves","title":"Related initiative: Current DWP workflow for importing PPFG (2 ea curves)","text":"<p>Contact: Christian Borsheim</p> <ul> <li> <p>EDM only has two containers for curves (1 ea for Pore Pressure and 1   ea for Fracture Gradient). The component \u201cSubsurface Essentials\u201d   allows importing of new revisions and rewriting of existing revisions.   The non-active curves are stored in PostGres. You may separate the   revisions (and hence different curves) by naming convention. Eg   \u201cMinimum Pore Pressure Rev1\u201d, superseeding \u201cExpected Collapse Pressure   Rev2.3 From Gunnar_2144124\u201d. This is not a sustainable solution, but   can be used in the short term.</p> </li> <li> <p>EDM does not support curves with ~500+ data points (TVD vs pressure):   the system crashes. Douglas\u2013Peucker algorithm is applied on import to   DWP (smoothening the curve).</p> </li> <li> <p>We can transfer PPFG and WBS today So, DWP can pick it up and   have been able to for ages. Pete Heavey will rightly not permit this   until the data governance (wellbore-PPFG linkage) issue in DWP is   resolved. Whether the BEST way is to take it from OW is debatable \u2013   now I believe it should come from Petrel Studio.</p> </li> </ul> <p>Below snapshot from the Import workflow in DWP.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"project/PPFG/#related-initiative-dwp-expansion-of-subsurface-essentials-component-with-more-ppfg-curves","title":"Related initiative: DWP expansion of Subsurface Essentials component with more PPFG curves","text":"<p>Contact: Christian Borsheim</p> <p>Currently (07.07.2024), DWP application is limited to 1 ea Pore Pressure curve and 1 ea Fracture Gradient curve. This has limitations towards the well engineering workflows, which ideally would allow for different combinations of curves to be used from all BMS-required curves (range of PP, range of FG, range of minimum horizontal stress (shmin), OBG, CG). Ideally, there would be a logic to extract \u201cthe worst case scenario\u201d out of a set of curves for input into the simulations (an example being a combined worst case of Pore Pressure and Collapse Gradient as the \u201clower pressure limit. For example, the Pore Pressure could be the lowest limit for the first 2000 mTVD and the Collapse Gradient could be the limit for the last 1000 mTVD of the well).</p> <p>In order to address this, the DWP R&amp;D team is working on an expansion of the Subsurface Essentials component to be deployed in August 2024 which allows for the manual import of all 11 curves. In the backend, this means that 9 additional containers are created in PostGres for the missing curves (this to be verified by DWP PO).</p> <p></p>"},{"location":"project/PPFG/#current-edm-schemas","title":"Current EDM schemas","text":"<ol> <li> <p>Container for WBS, OBG, Shmin (or any range of PP, FG or previously     mentioned); does not exist.</p> </li> <li> <p>Container for Faults is the same as Formations.</p> </li> <li> <p>No container for UCS, Friction angle, PVT data identified yet. PVT     data is used in BOWK suite.</p> </li> </ol>"},{"location":"project/PPFG/#current-edm-schemas-trajectory","title":"Current EDM schemas: Trajectory","text":""},{"location":"project/PPFG/#current-edm-schemas-ppfg","title":"Current EDM schemas: PPFG","text":""},{"location":"project/PPFG/#current-edm-schemas-formations","title":"Current EDM schemas: Formations","text":"<p>mg src=\"./images/PPFG-14.png\" style=\"width:4.54167in;height:6.5in\" a a=\"A screenshot of a computer Description automatically generated\" /&gt;</p>"},{"location":"project/PPFG/#list-of-required-data","title":"List of required data","text":"<p>In general, all are repeatable objects unless specified a singular object.</p> \u201cData Product\u201d The data it includes Current status Metadata <ul> <li><p>Time created</p></li> <li><p>Time share</p></li> <li><p>Author</p></li> <li><p>Last change</p></li> <li><p>Last change time</p></li> <li><p>Last change author</p></li> <li><p>Schema KIND</p></li> <li><p>Schema version</p></li> <li><p>Schema owner</p></li> <li><p>Schema created date</p></li> <li><p>Schema last update time</p></li> <li><p>Schema last update author</p></li> <li><p>Name</p></li> <li><p>Description</p></li> </ul> Standard metadata. General well info (well name, wellbore)\u00a0(singular entity) <ul> <li><p>Well ID EDM</p></li> <li><p>Wellbore ID EDM</p></li> <li><p>Well ID FDP</p></li> <li><p>Wellbore ID FDP</p></li> <li><p>Well name\u00a0</p></li> <li><p>Wellbore name\u00a0</p></li> </ul> System of Reference is incomplete. We can use EDM ID as \u201cleast bad solution\u201d now. Location data (singular entity) <ul> <li><p>Basic well data (location, datum, surface location)</p></li> <li><p>Relief well locations</p></li> <li><p>Offset Wells</p></li> <li><p>CRS</p></li> <li><p>Datum</p></li> <li><p>UTM</p></li> <li><p>Grid/True North</p></li> </ul> <p>Can come from CWP or manually in COMPASS.</p> <p>Offset Wells Analyzer potential source for offset wells listing.</p> Well Trajectory\u00a0(singular entity) <ul> <li><p>Measured depth\u00a0</p></li> <li><p>Inclination\u00a0</p></li> <li><p>Azimuth\u00a0</p></li> <li><p>True vertical depth\u00a0</p></li> <li><p>Northing\u00a0</p></li> <li><p>Easting\u00a0</p></li> </ul> Can come from CWP or manually in COMPASS. Pressure Profile\u00a0(singular entity) <ul> <li><p>Datum references</p></li> <li><p>Measured depth [m]\u00a0</p></li> <li><p>True vertical Depth [m]\u00a0</p></li> <li><p>Fracture Pressure [kg/m^3]\u00a0 min</p></li> <li><p>Fracture Pressure [kg/m^3]\u00a0 ML</p></li> <li><p>Fracture Pressure [kg/m^3]\u00a0 max</p></li> <li><p>Pore Pressure [kg/m^3]\u00a0min</p></li> <li><p>Pore Pressure [kg/m^3]\u00a0ML</p></li> <li><p>Pore Pressure [kg/m^3]\u00a0max</p></li> <li><p>Minimum Horizontal Stress min</p></li> <li><p>Minimum Horizontal Stress ML</p></li> <li><p>Minimum Horizontal Stress max</p></li> <li><p>Collapse Pressure [kg/m^3]\u00a0</p></li> <li><p>Overburden Gradient</p></li> <li><p>References to model_id, author, date etc..</p></li> <li></li> </ul> Put in manually in DWP (Stresscheck)"},{"location":"project/PPFG/#proposed-schema","title":"Proposed schema","text":"<p>Note: PPFGDataset is defined by OSDU as a \"Work Product Component\". This means it is a json schema resting on top of conventional files. Allowable file types for PPFGDataset.1.2.0: WITSML, LAS2, LAS3, csv.</p> <p>https://community.opengroup.org/osdu/data/data-definitions/-/blob/master/E-R/work-product-component/PPFGDataset.1.2.0.md</p> <ul> <li> <p>Attributes: In general, use OSDU WKS. Attributes will be mTVD   against pressure for 1 pore pressure and 1 fracture curve at is most   barebone. At its best, it will include a range of pore and fracture   gradients, authors, author date, lineage information, datum   information, legal and access, version, confidence, data quality   measures, technical maturity, allowable useage areas, etc..</p> </li> <li> <p>data types: json, csv or reference to source. \"data-savy\" people   to decide; not domain experts</p> </li> <li> <p>constraints: access rights, region, tight groups for exploration   wells, must be tied to trajectory</p> </li> <li> <p>relationships to other elements: trajectory and datum (which again   are related to the rest of the Wellbore hierarchy)</p> </li> <li> <p>relationships to other elements in the data unit (e.g. functional   dependencies): dependent on changes and updates to the simulated   model of the asset and trajectory updates.</p> </li> <li> <p>Include maturity information according to \"51-001619 PPFG review   template\": Well type, project stage (DG)</p> </li> <li> <p>Analysis type and Application: 1D, 2D, 3D, basin, etc.. in Drilworks,   Petrel, Unix, etc..</p> </li> <li> <p>List key offset wells and data types from these offset wells.</p> </li> <li> <p>Necessary with Overburden versus Reservoir separations (ref. Valhall)?</p> </li> <li> <p>Connection to model_ID</p> </li> <li> <p>Need to be specific on WHICH PPFG we want (latest or the one used for   Well Design)</p> </li> <li> <p>-&gt; OpenWorks creates new IDs when received from Petrel.</p> </li> <li> <p>Number of points? Every 1,5,10 meters?</p> </li> </ul> <p> </p> <p></p>"},{"location":"project/PPFG/#proposed-alternatives","title":"Proposed alternatives","text":"<p>There are many roads to Rome. Future DataOps to make a decision on Cost/Timeline.</p> <p></p>"},{"location":"project/PPFG/#alternative-a-pull-from-openworks","title":"Alternative A: Pull from OpenWorks","text":"<p>DWP already has a solution for pulling 1 Pore Pressure and 1 Fracture Gradient curve from DSG (DecisionSpace Geosciences application by Halliburton). This functionality was halted due to improper governance: User could tie any PPFG from any well to the Design they were working on.</p> <p>This solution proposes an upgrade to that code to only be able to pull curves that are consistent with the Design_ID that the engineer is currently working on. In order to do this, we need amongst others, the trajectory_ID (can be represented by EDM def_survey_header_ID) to be present on the curves as metadata.</p> <p>It is likely that it is possible to expand such a solution (OW pull to DWP) to other initiatives? OW curves are available through DSIS. Coupled with the same limitations to the OW query (design-locked); this should be feasible.</p> <p>There is an argument to move PPFG not only from Petrel to EDM directly, but also to OW. \u201cWe would like to be able to use this data, especially the collapse data, but other properties in 2d and 3d such as UCS in CWP. Primarily for multidisciplinary visualisation. Barriers to doing this have been software instability in DSG for the last months, and unavailability of the Petrel PPFG/RM models.\u201d</p> <p>PPFG should be considered in context of the CWP session. It makes sense for the author (Henrik N\u00e6sgaard) to define a loop for trajectory and then tie all Data products to the same loop. In other words, when there is a change in the trajectory, Petrel Studio publishes a new set of PPFG, OBG, CG, Shmin, UCS, FA, Picks formations, Picks faults.</p> <p> </p>"},{"location":"project/PPFG/#alternative-b-reverse-engineer-rocq-connector","title":"Alternative B: Reverse-Engineer ROCQ Connector","text":""},{"location":"project/PPFG/#alternative-c-whole-new-solution-example-backbone-or-adme","title":"Alternative C: Whole new solution (example Backbone or ADME)","text":""},{"location":"project/PPFG/#alternative-d-data-product-version","title":"Alternative D: Data Product version","text":"<p>By Data Product Support Team.</p> <p>AS-IS data architecture</p> <p></p> <ul> <li>For a more detailed description, miro board from \u2018walkthrough 23 Sep\u2019.   By Albert Kingma.</li> </ul> <p></p> <p>TRANSITION data architecture (MVP)</p> <p>Proposed MVP described below. Use case: Christian Borsheim; Transfer PPFG data from Petrel to DWP.</p> <p>The solution is broken down into 5 elements:</p> <ol> <li> <p>Standard output from Delfi/Petrel.</p> </li> <li> <p>Extract data</p> </li> <li> <p>Transform data to OSDU compliant format</p> </li> <li> <p>Store data</p> </li> <li> <p>Consume data in DWP</p> </li> </ol> <p></p> <p>Hi-level description of solution. Bold means short term solution.</p> <ol> <li> <p>Expose PPFG data from Petrel to Petrel Studio</p> <ol> <li> <p>No excel sheet to be exported.</p> </li> <li> <p>Prepare and QA/QC data from Petrel in a standardized format.     Rock Mechanics &amp; Geology Ops.</p> </li> <li> <p>Provide a standard format/schema with naming convention -&gt;     PeteH, Samad, Robert Maclean</p> </li> <li> <p>QA/QC by Data Manager (tag the data for exposure) -&gt; Robert     Maclean.</p> </li> <li> <p>Also need to test data from Delfi.</p> </li> </ol> </li> <li> <p>Extract PPFG data from Petrel Studio</p> <ol> <li> <p>Testing with Cognuite API to extract data</p> </li> <li> <p>Alternatives: 1) via Cognuite API (short term) 2) via     SLB SDK (long long term) 3) via S-drive (not recommended) 4)     others</p> </li> </ol> </li> <li> <p>Do necessary transformations</p> <ol> <li> <p>Guided by OSDU, map data into Well Known Schema.</p> </li> <li> <p>Alternatives: 1) Cognuit (not recommended), 2) HAL     (investigate) 3) ABP azure pipeline via python     notebook 4) via Nitro platform.</p> </li> </ol> </li> <li> <p>Publish and Share data in a data hub</p> <ol> <li> <p>Preferable HAL OSDU instance. Make design choices to maximize     OSDU adaptation.</p> </li> <li> <p>Alternatives: 1) Direct with HAL ADME api\u2019s. OR 2) via ABP APIM</p> </li> </ol> </li> <li> <p>DWP to consume PPFG data</p> <ol> <li>Alternatives 1) DWP read from HAL ADME. 2) DWP read from     APIM. 3) DWP to manually import excel sheet.</li> </ol> </li> </ol> <p>TO-BE data architecture</p> <p>The below diagram is used for discussions. Some strategic decisions need to be made.</p> <ul> <li>See FutureOps miro board for original diagram.</li> </ul> <p></p> <p>Discussion:</p> <ul> <li> <p>Diagram show a possible data architecture with 3 ADME instances.</p> </li> <li> <p>The idea is to visualise how to share data between tenants via APIM   APIs.</p> </li> <li> <p>Concern: how do we keep data in sync if it is duplicated in the     tenants?</p> </li> <li> <p>The data products are placed according to \u2018governing tenant\u2019</p> </li> <li> <p>Investigate a version without APIM, that has direct access to HADME   api\u2019s.</p> </li> <li> <p>Investigate the role of Gravitas Edge, used to visualised PPFG curves   for ExpRes. How is data exported into Gravitas?</p> </li> <li> <p>Data architecture: should investigate which pattern to use. Is Event   Hub an option? Also see this guideline page and questions.</p> </li> <li> <p>Is PPFG excel version exported in LAS format? It seems so.</p> </li> </ul>"},{"location":"project/PPFG/#alternative-e-grants-perspective","title":"Alternative E: Grant\u2019s perspective","text":"<p>This diagram is from Grant and describes the larger picture. For PPFG dataset, the data flow can be;</p> <ol> <li> <p>Petrel generates PPFG curves.</p> </li> <li> <p>PPFG workflow is run in Petrel.</p> </li> <li> <p>RoQC Governance Tool ensure that the naming standards are compliant.</p> </li> <li> <p>The data is pushed into Studio (manually?).</p> </li> <li> <p>Cognuit pulls data once it is tagged \u2018approved\u2019 by a Data Manager.</p> </li> <li> <p>Then Cognuit can push data into OpenWorks DB, , which again push     data to the \u2018master\u2019 EDM DB, but since EDM can only store 2 PPFG     curves (of 11 or more), a way around is to store a full dataset in     DWP\u2019s PostgreSQL DB. This may be great for DWP, but solution must be     able to scale into other consumers too. The goal is to store in an     ADME instance. It really should store the PPFG data in SLB\u2019s ADME,     as they are the owner of the data set, then Hal pulls data from this     tenant (the future Microsoft Mesh). Do we have any interim solution     today? (APIM?)</p> </li> </ol> <p>Discussion with Kjetil\u00d8:</p> <ul> <li> <p>Should get as close to source as possible. Export directly from petrel   /governance plugin to xADME?</p> </li> <li> <p>Need to ensure xADME contains sufficient rec data such as wellbore etc   (dependencies).</p> </li> </ul>"},{"location":"user-guide/how-to-deploy-docs-on-digipedia/","title":"How-to-Guide - Deployment to Digipedia and Github Pages","text":"<p>Last updated November 1st, 2024 | Author: Jon Steinar Folstad</p>"},{"location":"user-guide/how-to-deploy-docs-on-digipedia/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have read the How-to-Guide - MkDocs Documentation, and your project follows this file structure:</p> <pre><code>your_project/\n\u2514\u2500\u2500 docs/\n    \u251c\u2500\u2500 build/                     # Output directory where HTML files are generated\n    \u2502   \u251c\u2500\u2500 html/                  # HTML files go here after building\n    \u2502   \u2514\u2500\u2500 doctrees/              # Stores intermediary files for fast rebuilds\n    \u2502\n    \u251c\u2500\u2500 source/                    # Source files directory\n    \u2502   \u251c\u2500\u2500 _static/               # Custom static files (CSS, JS, images, etc.)\n    \u2502   \u251c\u2500\u2500 _templates/            # Custom templates (e.g., for modifying HTML layout)\n    \u2502   \u251c\u2500\u2500 conf.py                # Main configuration file for Sphinx\n    \u2502   \u2514\u2500\u2500 index.rst              # Main document (typically the homepage of your docs)\n    \u2502\n    \u251c\u2500\u2500 Makefile                   # Build file for Unix/Linux\n    \u2514\u2500\u2500 make.bat                   # Build file for Windows\n</code></pre> <p>If your file structure deviates from the above, you may need to modify the workflow files located in <code>.github/workflows/</code> to ensure successful documentation generation deployment. Adjust the paths in the workflow to match your project structure if needed.</p>"},{"location":"user-guide/how-to-deploy-docs-on-digipedia/#generate-ssh-deploy-key-in-terminal","title":"Generate SSH deploy key in terminal","text":"<p>To generate an SSH deploy key specifically for pushing from one repository to another repository, follow these steps:</p> <ol> <li>Generate a New SSH Key Pair</li> </ol> <p>Open a terminal on your Linux machine and run the following command to generate a new SSH key pair:</p> <pre><code>ssh-keygen -t rsa -b 4096 -f ./id_rsa\n</code></pre> <p>When prompted to \"Enter a file in which to save the key,\" you can press Enter to accept the default location or specify a different path. When prompted for a passphrase, you can either enter a secure passphrase or leave it empty for no passphrase.</p> <p>To add any comment to ssh pair use flag <code>-c &lt;add_your_comment&gt;</code></p> <ol> <li>Add the private key to Source repository (your project repository) and Public Key to the Target Repository dig-handbook-mkdocs</li> </ol> <p>Copy the private key to your clipboard:</p> <pre><code>cat ~/.ssh/id_rsa\n</code></pre> <p>Copy the public key to your clipboard:</p> <pre><code>cat ~/.ssh/id_rsa.pub\n</code></pre> <p>Follow these steps to add the Private Key to your project repository</p> <ol> <li>Navigate to your GitHub repository where the workflow is defined.</li> <li>Click on the \"Settings\" tab.</li> <li>In the left sidebar, click on \"Secrets and variables\" and then \"Actions.\"</li> <li>Click on \"New repository secret.\"</li> <li>Name the secret SSH_DEPLOY_KEY_DIGIPEDIA.</li> <li>Paste the contents of your private key into the \"Value\" field.</li> <li>Click \"Add secret.\"</li> </ol> <p>Note Your ssh private key begines with '-----BEGIN OPENSSH PRIVATE KEY-----', and it ends with '-----END OPENSSH PRIVATE KEY-----'.</p> <p>Then, add the public key to the target repository:</p> <ol> <li>Navigate to the target repository on GitHub.</li> <li>Click on the \"Settings\" tab.</li> <li>In the left sidebar, click on \"Deploy keys.\"</li> <li>Click on \"Add deploy key.\"</li> <li>Give your key a title, paste the public key into the \"Key\" field, and check the \"Allow write access\" box if necessary.</li> <li>Click \"Add key.\"</li> </ol> <p>Note Your ssh public key begins with 'ssh-rsa', 'ecdsa-sha2-nistp256', 'ecdsa-sha2-nistp384', 'ecdsa-sha2-nistp521', 'ssh-ed25519', 'sk-ecdsa-sha2-nistp256@openssh.com', or 'sk-ssh-ed25519@openssh.com'.</p>"},{"location":"user-guide/how-to-deploy-docs-on-digipedia/#add-documentation-to-digipedia-and-github-pages","title":"Add Documentation to DigiPedia and GitHub Pages","text":"<p>Distribute your project documentation to DigiPedia and GitHub Pages with these steps.</p>"},{"location":"user-guide/how-to-deploy-docs-on-digipedia/#deploy-docs-to-digipedia","title":"Deploy Docs to DIGiPedia","text":"<ol> <li> <p>Add Workflow    Copy the <code>gh-copy-html-to-repo.yaml</code> file to <code>.github/workflows/</code> in your project root.</p> </li> <li> <p>Contact for Details    Email jon.steinar.folstad@akerbp.com from Data Science &amp; Analytics to obtain DigiPedia destination details.</p> </li> <li> <p>Configure Workflow    In <code>gh-copy-html-to-repo.yaml</code>, modify <code>&lt;BU&gt;</code> with the destination information provided, and <code>&lt;repository-name&gt;</code> with the name of your repository. Also, modify <code>user-email</code>, <code>user-name</code> and <code>commit-message</code> to reflect your user information and repository changes.</p> </li> </ol> <pre><code>printf \"nav:\\n  - README.md\\n  - docs: ./code/&lt;BU&gt;/&lt;repository-name&gt;/docs/\" &gt; docs/build/publish/.pages\n\nsource-directory: docs/build/publish\ndestination-github-username: \"AkerBP\"\ndestination-repository-name: \"dig-handbook-mkdocs\"\ntarget-branch: git-docs\ntarget-directory: docs/code/&lt;BU&gt;/$\\{\\{ github.event.repository.name \\}\\}\nuser-email: &lt;your user email adress&gt;\nuser-name: &lt;your github user-name&gt;\ncommit-message: \"[GHA] Update &lt;repository name&gt; docs html files.\"\n</code></pre> <ol> <li>Check Deployment</li> </ol> <p>Go to the Digipedia GitHub repository to verify that your files are present in the <code>docs/code</code> directory.</p> <p>If the deployment was successful, you can view your documentation on DIGipedia Pages.</p> <p>Note: This DIGpedia page is a copy of the original DIGpedia page. You would be notified when these two pages would merge together.</p>"},{"location":"user-guide/how-to-deploy-docs-on-digipedia/#deploy-docs-to-github-pages","title":"Deploy docs to Github Pages","text":"<ol> <li>Add and configure Workflow Modify the workflow file with necessary information for GitHub Pages deployment.</li> </ol> <p>Add <code>gh-pages-deploy-sphinx.yaml</code>` to the folder .github/workflows/ on project root.</p> <ol> <li>Check Deployment Go to your repository's Settings tab, select Pages from the sidebar, and visit the provided URL to confirm your site is live.</li> </ol>"},{"location":"user-guide/how-to-deploy-docs-on-digipedia/#references","title":"References","text":"<ul> <li>Documentation destination</li> <li>Deploy to DIGipedia</li> <li>Deploy to Github Pages</li> </ul>"},{"location":"user-guide/how-to-setup-mkdocs/","title":"How to setup mkdocs with existing repository","text":"<p>We are using mkdocs with material plugin and installing material plugin will automatically install compatible versions of all dependencies: MkDocs, Markdown, Pygments and Python Markdown Extensions. Material for MkDocs always strives to support the latest versions, so there's no need to install those packages separately.</p> <p>mkdocs-material documentation &gt;&gt;&gt;</p>"},{"location":"user-guide/how-to-setup-mkdocs/#installation-of-dependencies-via-pip","title":"Installation of dependencies via pip","text":"<p><code>pip install mkdocs-material mkdocs-coverage mkdocstrings-python mkdocs-gen-files</code></p>"},{"location":"user-guide/how-to-setup-mkdocs/#sample-pyprojecttoml","title":"Sample pyproject.toml","text":"pyproject.toml <pre><code>[tool.poetry]\nname = \"mkdocs101\"\nversion = \"0.1.0\"\ndescription = \"Test project for mkdocs documentation setup\"\nauthors = [\"XYZ\"]\nlicense = \"MIT\"\nreadme = \"README.md\"\n\n[tool.poetry.dependencies]\npython = \"^3.12\"\nmkdocs-material = \"^9.5.45\"\nmkdocs-coverage = \"^1.1.0\"\nmkdocstrings-python = \"^1.12.2\"\nmkdocs-gen-files = \"^0.5.0\"\n\n[tool.poetry.group.dev.dependencies]\nblack = \"^24.10.0\"\nflake8 = \"^7.1.1\"\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n</code></pre>"},{"location":"user-guide/how-to-setup-mkdocs/#getting-started-with-mkdocs","title":"Getting started with mkdocs","text":""},{"location":"user-guide/how-to-setup-mkdocs/#alternative-1","title":"Alternative 1","text":"<p>Warning</p> <p>Make sure no folder is named <code>docs</code> before running following command to prevent data loss</p> <p><code>mkdocs new .</code> will create <code>docs</code> folder and <code>mkdocs.yaml</code> configuration file at root</p> <p>Folder structure: <pre><code>|   .gitignore\n|   mkdocs.yml\n|   poetry.lock\n|   pyproject.toml\n|   README.md\n+---docs\n        index.md\n</code></pre></p>"},{"location":"user-guide/how-to-setup-mkdocs/#alternative-2","title":"Alternative 2","text":"<p>In case of having a <code>docs</code> folder already in the repository</p> <p><code>mkdocs new documentation</code> will nest <code>docs</code> folder and <code>mkdocs.yaml</code> under given folder name which is <code>documentation</code> in this case.</p> <p>Folder structure: <pre><code>|   .gitignore\n|   poetry.lock\n|   pyproject.toml\n|   README.md\n+---documentation\n    |   mkdocs.yml\n    +---docs\n            index.md\n</code></pre></p> <p>Info</p> <p>This guideline will follow the first alternative to create a new project, please remember to update paths with nested structure if second alternative was selected.</p> <p>At this very stage a basic documentation site can be seen by running <code>mkdocs serve</code> command which will build and start hosting documentation as static html site in localhost.</p>"},{"location":"user-guide/how-to-setup-mkdocs/#extensions-and-styling-digipedia-style","title":"Extensions and Styling (Digipedia style)","text":"<p>Adding some useful extensions like admonition, katex, mermaid etc. and styling to meet digipedia look and feel by using following <code>mkdocs.yaml</code> config.</p> mkdocs.yaml <pre><code>site_name: AkerBP Project &amp; Code Documentation\nsite_url: https://mydomain.org/AkerBP\nrepo_name: AkerBP/DataOps/dig-docs\nrepo_url: https://dev.azure.com/akerbp/DataOps/_git/dig-docs\n\n\ntheme:\nname: material\n# name: readthedocs\nfavicon: assets/images/favicon.ico\nlogo: assets/images/favicon.ico\nfont:\n    text: Lato\npalette:\n    # Palette toggle for light mode\n    - media: \"(prefers-color-scheme: light)\"\n    scheme: default\n    primary: akerbp\n    accent: akerbp\n    toggle:\n        icon: material/weather-night\n        name: Switch to dark mode\n\n    # Palette toggle for dark mode\n    - media: \"(prefers-color-scheme: dark)\"\n    scheme: slate\n    primary: akerbp\n    accent: akerbp\n    toggle:\n        icon: material/weather-sunny\n        name: Switch to light mode\nfeatures:\n    - navigation.footer\n    # - navigation.tabs\n    # - navigation.tabs.sticky\n    - navigation.sections\n    - navigation.indexes\n    - content.action.edit\n    - content.action.view\n    - content.code.copy\n    - toc.follow\n    - toc.integrate   # Integrate outline on the right navigation bar into left panel\n\nicon:\n    repo: fontawesome/brands/git-alt\n    edit: material/pencil\n    view: material/eye\n\nextra_javascript:\n- assets/javascripts/katex.js\n- https://unpkg.com/katex@0/dist/katex.min.js\n- https://unpkg.com/katex@0/dist/contrib/auto-render.min.js\n\n# Extra styling\nextra_css:\n- assets/stylesheets/extra.css\n- assets/stylesheets/img_carousel.css\n- https://unpkg.com/katex@0/dist/katex.min.css\n\nmarkdown_extensions:\n- admonition\n- attr_list\n- md_in_html\n- pymdownx.arithmatex:\n    generic: true\n- pymdownx.blocks.caption\n- pymdownx.caret\n- pymdownx.details\n- pymdownx.emoji:\n    emoji_index: !!python/name:material.extensions.emoji.twemoji\n    emoji_generator: !!python/name:material.extensions.emoji.to_svg\n- pymdownx.highlight:\n    anchor_linenums: true\n    line_spans: __span\n    pygments_lang_class: true\n- pymdownx.inlinehilite\n- pymdownx.mark\n- pymdownx.snippets\n- pymdownx.superfences:\n    custom_fences:\n        - name: mermaid\n        class: mermaid\n        format: !!python/name:pymdownx.superfences.fence_code_format\n- pymdownx.tabbed:\n    alternate_style: true\n- pymdownx.tasklist:\n    custom_checkbox: true\n- pymdownx.tilde\n- tables\n- toc:\n    permalink: true\n    permalink_title: \"Direct link to this heading\"\n    slugify: !!python/object/apply:pymdownx.slugs.slugify\n        kwds:\n        case: lower\n    toc_depth: 3\n</code></pre>"},{"location":"user-guide/mkdocs-basics/","title":"Admonitions","text":""},{"location":"user-guide/mkdocs-basics/#basic-admonition","title":"Basic Admonition","text":"Basic admonition <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"user-guide/mkdocs-basics/#inline-admonition","title":"Inline admonition","text":"<p>Lorem ipsum</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. This is an admonition comes to the end of line (inline) Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. This is an admonition comes to the end of line (inline) Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Lorem ipsum</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Dolor esse officia mollit aute veniam. Sit aute id cillum nisi minim adipisicing exercitation est proident sit. Cillum quis irure sit duis ullamco. Aliquip et magna nulla anim culpa velit dolore do cupidatat fugiat laborum magna. Commodo non non cupidatat fugiat laborum irure aliqua duis. Incididunt aliquip officia voluptate irure irure tempor reprehenderit velit nisi excepteur. Commodo commodo labore excepteur elit reprehenderit et.</p> <p>For more of admonitions...</p>"},{"location":"user-guide/mkdocs-basics/#code-blocks","title":"Code blocks","text":"bubble_sort.py<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"user-guide/mkdocs-basics/#content-tabs","title":"Content tabs","text":"CC++ <pre><code>#include &lt;stdio.h&gt;\n\nint main(void) {\n  printf(\"Hello world!\\n\");\n  return 0;\n}\n</code></pre> <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre> <pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre>"},{"location":"user-guide/mkdocs-basics/#diagrams","title":"Diagrams","text":""},{"location":"user-guide/mkdocs-basics/#formatting","title":"Formatting","text":"<ul> <li>This was marked (highlight)</li> <li>This was inserted (underline)</li> <li>This was deleted (strikethrough)</li> </ul>"},{"location":"user-guide/mkdocs-basics/#grids","title":"Grids","text":"<ul> <li> <p> Set up in 5 minutes</p> <p>Install <code>mkdocs-material</code> with <code>pip</code> and get up and running in minutes</p> <p> Getting started</p> </li> <li> <p> It's just Markdown</p> <p>Focus on your content and generate a responsive and searchable static site</p> <p> Reference</p> </li> <li> <p> Made to measure</p> <p>Change the colors, fonts, language, icons, logo and more with a few lines</p> <p> Customization</p> </li> <li> <p> Open Source, MIT</p> <p>Material for MkDocs is licensed under MIT and available on [GitHub]</p> <p> License</p> </li> </ul>"},{"location":"user-guide/mkdocs-basics/#math","title":"Math","text":"\\[ \\cos x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{(2k)!}x^{2k} \\]"}]}